
<!doctype html>
<html lang="en" class="no-js">
  <head>
    
      <meta charset="utf-8">
      <meta name="viewport" content="width=device-width,initial-scale=1">
      
      
      
        <link rel="canonical" href="https://pytorch.apachecn.org/2.0/docs/notes/cuda/">
      
      
        <link rel="prev" href="../cpu_threading_torchscript_inference/">
      
      
        <link rel="next" href="../ddp/">
      
      
      <link rel="icon" href="../../../../assets/images/favicon.png">
      <meta name="generator" content="mkdocs-1.5.3, mkdocs-material-9.5.10">
    
    
      
        <title>CUDA semantics - 【布客】PyTorch 中文翻译</title>
      
    
    
      <link rel="stylesheet" href="../../../../assets/stylesheets/main.7e359304.min.css">
      
      
  
  
    
    
  
    
    
  
    
    
  
    
    
  
    
    
  
    
    
  
    
    
  
    
    
  
    
    
  
    
    
  
    
    
  
    
    
  
  
  <style>:root{--md-admonition-icon--note:url('data:image/svg+xml;charset=utf-8,<svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 16 16"><path d="M1 7.775V2.75C1 1.784 1.784 1 2.75 1h5.025c.464 0 .91.184 1.238.513l6.25 6.25a1.75 1.75 0 0 1 0 2.474l-5.026 5.026a1.75 1.75 0 0 1-2.474 0l-6.25-6.25A1.752 1.752 0 0 1 1 7.775Zm1.5 0c0 .066.026.13.073.177l6.25 6.25a.25.25 0 0 0 .354 0l5.025-5.025a.25.25 0 0 0 0-.354l-6.25-6.25a.25.25 0 0 0-.177-.073H2.75a.25.25 0 0 0-.25.25ZM6 5a1 1 0 1 1 0 2 1 1 0 0 1 0-2Z"/></svg>');--md-admonition-icon--abstract:url('data:image/svg+xml;charset=utf-8,<svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 16 16"><path d="M2.5 1.75v11.5c0 .138.112.25.25.25h3.17a.75.75 0 0 1 0 1.5H2.75A1.75 1.75 0 0 1 1 13.25V1.75C1 .784 1.784 0 2.75 0h8.5C12.216 0 13 .784 13 1.75v7.736a.75.75 0 0 1-1.5 0V1.75a.25.25 0 0 0-.25-.25h-8.5a.25.25 0 0 0-.25.25Zm13.274 9.537v-.001l-4.557 4.45a.75.75 0 0 1-1.055-.008l-1.943-1.95a.75.75 0 0 1 1.062-1.058l1.419 1.425 4.026-3.932a.75.75 0 1 1 1.048 1.074ZM4.75 4h4.5a.75.75 0 0 1 0 1.5h-4.5a.75.75 0 0 1 0-1.5ZM4 7.75A.75.75 0 0 1 4.75 7h2a.75.75 0 0 1 0 1.5h-2A.75.75 0 0 1 4 7.75Z"/></svg>');--md-admonition-icon--info:url('data:image/svg+xml;charset=utf-8,<svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 16 16"><path d="M0 8a8 8 0 1 1 16 0A8 8 0 0 1 0 8Zm8-6.5a6.5 6.5 0 1 0 0 13 6.5 6.5 0 0 0 0-13ZM6.5 7.75A.75.75 0 0 1 7.25 7h1a.75.75 0 0 1 .75.75v2.75h.25a.75.75 0 0 1 0 1.5h-2a.75.75 0 0 1 0-1.5h.25v-2h-.25a.75.75 0 0 1-.75-.75ZM8 6a1 1 0 1 1 0-2 1 1 0 0 1 0 2Z"/></svg>');--md-admonition-icon--tip:url('data:image/svg+xml;charset=utf-8,<svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 16 16"><path d="M3.499.75a.75.75 0 0 1 1.5 0v.996C5.9 2.903 6.793 3.65 7.662 4.376l.24.202c-.036-.694.055-1.422.426-2.163C9.1.873 10.794-.045 12.622.26 14.408.558 16 1.94 16 4.25c0 1.278-.954 2.575-2.44 2.734l.146.508.065.22c.203.701.412 1.455.476 2.226.142 1.707-.4 3.03-1.487 3.898C11.714 14.671 10.27 15 8.75 15h-6a.75.75 0 0 1 0-1.5h1.376a4.484 4.484 0 0 1-.563-1.191 3.835 3.835 0 0 1-.05-2.063 4.647 4.647 0 0 1-2.025-.293.75.75 0 0 1 .525-1.406c1.357.507 2.376-.006 2.698-.318l.009-.01a.747.747 0 0 1 1.06 0 .748.748 0 0 1-.012 1.074c-.912.92-.992 1.835-.768 2.586.221.74.745 1.337 1.196 1.621H8.75c1.343 0 2.398-.296 3.074-.836.635-.507 1.036-1.31.928-2.602-.05-.603-.216-1.224-.422-1.93l-.064-.221c-.12-.407-.246-.84-.353-1.29a2.425 2.425 0 0 1-.507-.441 3.075 3.075 0 0 1-.633-1.248.75.75 0 0 1 1.455-.364c.046.185.144.436.31.627.146.168.353.305.712.305.738 0 1.25-.615 1.25-1.25 0-1.47-.95-2.315-2.123-2.51-1.172-.196-2.227.387-2.706 1.345-.46.92-.27 1.774.019 3.062l.042.19a.884.884 0 0 1 .01.05c.348.443.666.949.94 1.553a.75.75 0 1 1-1.365.62c-.553-1.217-1.32-1.94-2.3-2.768L6.7 5.527c-.814-.68-1.75-1.462-2.692-2.619a3.737 3.737 0 0 0-1.023.88c-.406.495-.663 1.036-.722 1.508.116.122.306.21.591.239.388.038.797-.06 1.032-.19a.75.75 0 0 1 .728 1.31c-.515.287-1.23.439-1.906.373-.682-.067-1.473-.38-1.879-1.193L.75 5.677V5.5c0-.984.48-1.94 1.077-2.664.46-.559 1.05-1.055 1.673-1.353V.75Z"/></svg>');--md-admonition-icon--success:url('data:image/svg+xml;charset=utf-8,<svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 16 16"><path d="M13.78 4.22a.75.75 0 0 1 0 1.06l-7.25 7.25a.75.75 0 0 1-1.06 0L2.22 9.28a.751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018L6 10.94l6.72-6.72a.75.75 0 0 1 1.06 0Z"/></svg>');--md-admonition-icon--question:url('data:image/svg+xml;charset=utf-8,<svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 16 16"><path d="M0 8a8 8 0 1 1 16 0A8 8 0 0 1 0 8Zm8-6.5a6.5 6.5 0 1 0 0 13 6.5 6.5 0 0 0 0-13ZM6.92 6.085h.001a.749.749 0 1 1-1.342-.67c.169-.339.436-.701.849-.977C6.845 4.16 7.369 4 8 4a2.756 2.756 0 0 1 1.637.525c.503.377.863.965.863 1.725 0 .448-.115.83-.329 1.15-.205.307-.47.513-.692.662-.109.072-.22.138-.313.195l-.006.004a6.24 6.24 0 0 0-.26.16.952.952 0 0 0-.276.245.75.75 0 0 1-1.248-.832c.184-.264.42-.489.692-.661.103-.067.207-.132.313-.195l.007-.004c.1-.061.182-.11.258-.161a.969.969 0 0 0 .277-.245C8.96 6.514 9 6.427 9 6.25a.612.612 0 0 0-.262-.525A1.27 1.27 0 0 0 8 5.5c-.369 0-.595.09-.74.187a1.01 1.01 0 0 0-.34.398ZM9 11a1 1 0 1 1-2 0 1 1 0 0 1 2 0Z"/></svg>');--md-admonition-icon--warning:url('data:image/svg+xml;charset=utf-8,<svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 16 16"><path d="M6.457 1.047c.659-1.234 2.427-1.234 3.086 0l6.082 11.378A1.75 1.75 0 0 1 14.082 15H1.918a1.75 1.75 0 0 1-1.543-2.575Zm1.763.707a.25.25 0 0 0-.44 0L1.698 13.132a.25.25 0 0 0 .22.368h12.164a.25.25 0 0 0 .22-.368Zm.53 3.996v2.5a.75.75 0 0 1-1.5 0v-2.5a.75.75 0 0 1 1.5 0ZM9 11a1 1 0 1 1-2 0 1 1 0 0 1 2 0Z"/></svg>');--md-admonition-icon--failure:url('data:image/svg+xml;charset=utf-8,<svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 16 16"><path d="M2.344 2.343h-.001a8 8 0 0 1 11.314 11.314A8.002 8.002 0 0 1 .234 10.089a8 8 0 0 1 2.11-7.746Zm1.06 10.253a6.5 6.5 0 1 0 9.108-9.275 6.5 6.5 0 0 0-9.108 9.275ZM6.03 4.97 8 6.94l1.97-1.97a.749.749 0 0 1 1.275.326.749.749 0 0 1-.215.734L9.06 8l1.97 1.97a.749.749 0 0 1-.326 1.275.749.749 0 0 1-.734-.215L8 9.06l-1.97 1.97a.749.749 0 0 1-1.275-.326.749.749 0 0 1 .215-.734L6.94 8 4.97 6.03a.751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018Z"/></svg>');--md-admonition-icon--danger:url('data:image/svg+xml;charset=utf-8,<svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 16 16"><path d="M9.504.43a1.516 1.516 0 0 1 2.437 1.713L10.415 5.5h2.123c1.57 0 2.346 1.909 1.22 3.004l-7.34 7.142a1.249 1.249 0 0 1-.871.354h-.302a1.25 1.25 0 0 1-1.157-1.723L5.633 10.5H3.462c-1.57 0-2.346-1.909-1.22-3.004L9.503.429Zm1.047 1.074L3.286 8.571A.25.25 0 0 0 3.462 9H6.75a.75.75 0 0 1 .694 1.034l-1.713 4.188 6.982-6.793A.25.25 0 0 0 12.538 7H9.25a.75.75 0 0 1-.683-1.06l2.008-4.418.003-.006a.036.036 0 0 0-.004-.009l-.006-.006-.008-.001c-.003 0-.006.002-.009.004Z"/></svg>');--md-admonition-icon--bug:url('data:image/svg+xml;charset=utf-8,<svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 16 16"><path d="M4.72.22a.75.75 0 0 1 1.06 0l1 .999a3.488 3.488 0 0 1 2.441 0l.999-1a.748.748 0 0 1 1.265.332.75.75 0 0 1-.205.729l-.775.776c.616.63.995 1.493.995 2.444v.327c0 .1-.009.197-.025.292.408.14.764.392 1.029.722l1.968-.787a.75.75 0 0 1 .556 1.392L13 7.258V9h2.25a.75.75 0 0 1 0 1.5H13v.5c0 .409-.049.806-.141 1.186l2.17.868a.75.75 0 0 1-.557 1.392l-2.184-.873A4.997 4.997 0 0 1 8 16a4.997 4.997 0 0 1-4.288-2.427l-2.183.873a.75.75 0 0 1-.558-1.392l2.17-.868A5.036 5.036 0 0 1 3 11v-.5H.75a.75.75 0 0 1 0-1.5H3V7.258L.971 6.446a.75.75 0 0 1 .558-1.392l1.967.787c.265-.33.62-.583 1.03-.722a1.677 1.677 0 0 1-.026-.292V4.5c0-.951.38-1.814.995-2.444L4.72 1.28a.75.75 0 0 1 0-1.06Zm.53 6.28a.75.75 0 0 0-.75.75V11a3.5 3.5 0 1 0 7 0V7.25a.75.75 0 0 0-.75-.75ZM6.173 5h3.654A.172.172 0 0 0 10 4.827V4.5a2 2 0 1 0-4 0v.327c0 .096.077.173.173.173Z"/></svg>');--md-admonition-icon--example:url('data:image/svg+xml;charset=utf-8,<svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 16 16"><path d="M5 5.782V2.5h-.25a.75.75 0 0 1 0-1.5h6.5a.75.75 0 0 1 0 1.5H11v3.282l3.666 5.76C15.619 13.04 14.543 15 12.767 15H3.233c-1.776 0-2.852-1.96-1.899-3.458Zm-2.4 6.565a.75.75 0 0 0 .633 1.153h9.534a.75.75 0 0 0 .633-1.153L12.225 10.5h-8.45ZM9.5 2.5h-3V6c0 .143-.04.283-.117.403L4.73 9h6.54L9.617 6.403A.746.746 0 0 1 9.5 6Z"/></svg>');--md-admonition-icon--quote:url('data:image/svg+xml;charset=utf-8,<svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 16 16"><path d="M1.75 2.5h10.5a.75.75 0 0 1 0 1.5H1.75a.75.75 0 0 1 0-1.5Zm4 5h8.5a.75.75 0 0 1 0 1.5h-8.5a.75.75 0 0 1 0-1.5Zm0 5h8.5a.75.75 0 0 1 0 1.5h-8.5a.75.75 0 0 1 0-1.5ZM2.5 7.75v6a.75.75 0 0 1-1.5 0v-6a.75.75 0 0 1 1.5 0Z"/></svg>');}</style>



    
    
      
    
    
      
        
        
        <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
        <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Roboto:300,300i,400,400i,700,700i%7CRoboto+Mono:400,400i,700,700i&display=fallback">
        <style>:root{--md-text-font:"Roboto";--md-code-font:"Roboto Mono"}</style>
      
    
    
    <script>__md_scope=new URL("../../../..",location),__md_hash=e=>[...e].reduce((e,_)=>(e<<5)-e+_.charCodeAt(0),0),__md_get=(e,_=localStorage,t=__md_scope)=>JSON.parse(_.getItem(t.pathname+"."+e)),__md_set=(e,_,t=localStorage,a=__md_scope)=>{try{t.setItem(a.pathname+"."+e,JSON.stringify(_))}catch(e){}}</script>
    
      

    
    
    
  </head>
  
  
    <body dir="ltr">
  
    
    <input class="md-toggle" data-md-toggle="drawer" type="checkbox" id="__drawer" autocomplete="off">
    <input class="md-toggle" data-md-toggle="search" type="checkbox" id="__search" autocomplete="off">
    <label class="md-overlay" for="__drawer"></label>
    <div data-md-component="skip">
      
        
        <a href="#cuda" class="md-skip">
          Skip to content
        </a>
      
    </div>
    <div data-md-component="announce">
      
    </div>
    
    
      

  

<header class="md-header md-header--shadow" data-md-component="header">
  <nav class="md-header__inner md-grid" aria-label="Header">
    <a href="../../../.." title="【布客】PyTorch 中文翻译" class="md-header__button md-logo" aria-label="【布客】PyTorch 中文翻译" data-md-component="logo">
      
  <img src="https://data.apachecn.org/img/logo/logo_green.png" alt="logo">

    </a>
    <label class="md-header__button md-icon" for="__drawer">
      
      <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M3 6h18v2H3V6m0 5h18v2H3v-2m0 5h18v2H3v-2Z"/></svg>
    </label>
    <div class="md-header__title" data-md-component="header-title">
      <div class="md-header__ellipsis">
        <div class="md-header__topic">
          <span class="md-ellipsis">
            【布客】PyTorch 中文翻译
          </span>
        </div>
        <div class="md-header__topic" data-md-component="header-topic">
          <span class="md-ellipsis">
            
              CUDA semantics
            
          </span>
        </div>
      </div>
    </div>
    
    
      <script>var media,input,key,value,palette=__md_get("__palette");if(palette&&palette.color){"(prefers-color-scheme)"===palette.color.media&&(media=matchMedia("(prefers-color-scheme: light)"),input=document.querySelector(media.matches?"[data-md-color-media='(prefers-color-scheme: light)']":"[data-md-color-media='(prefers-color-scheme: dark)']"),palette.color.media=input.getAttribute("data-md-color-media"),palette.color.scheme=input.getAttribute("data-md-color-scheme"),palette.color.primary=input.getAttribute("data-md-color-primary"),palette.color.accent=input.getAttribute("data-md-color-accent"));for([key,value]of Object.entries(palette.color))document.body.setAttribute("data-md-color-"+key,value)}</script>
    
    
    
      <label class="md-header__button md-icon" for="__search">
        
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M9.5 3A6.5 6.5 0 0 1 16 9.5c0 1.61-.59 3.09-1.56 4.23l.27.27h.79l5 5-1.5 1.5-5-5v-.79l-.27-.27A6.516 6.516 0 0 1 9.5 16 6.5 6.5 0 0 1 3 9.5 6.5 6.5 0 0 1 9.5 3m0 2C7 5 5 7 5 9.5S7 14 9.5 14 14 12 14 9.5 12 5 9.5 5Z"/></svg>
      </label>
      <div class="md-search" data-md-component="search" role="dialog">
  <label class="md-search__overlay" for="__search"></label>
  <div class="md-search__inner" role="search">
    <form class="md-search__form" name="search">
      <input type="text" class="md-search__input" name="query" aria-label="Search" placeholder="Search" autocapitalize="off" autocorrect="off" autocomplete="off" spellcheck="false" data-md-component="search-query" required>
      <label class="md-search__icon md-icon" for="__search">
        
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M9.5 3A6.5 6.5 0 0 1 16 9.5c0 1.61-.59 3.09-1.56 4.23l.27.27h.79l5 5-1.5 1.5-5-5v-.79l-.27-.27A6.516 6.516 0 0 1 9.5 16 6.5 6.5 0 0 1 3 9.5 6.5 6.5 0 0 1 9.5 3m0 2C7 5 5 7 5 9.5S7 14 9.5 14 14 12 14 9.5 12 5 9.5 5Z"/></svg>
        
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M20 11v2H8l5.5 5.5-1.42 1.42L4.16 12l7.92-7.92L13.5 5.5 8 11h12Z"/></svg>
      </label>
      <nav class="md-search__options" aria-label="Search">
        
        <button type="reset" class="md-search__icon md-icon" title="Clear" aria-label="Clear" tabindex="-1">
          
          <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M19 6.41 17.59 5 12 10.59 6.41 5 5 6.41 10.59 12 5 17.59 6.41 19 12 13.41 17.59 19 19 17.59 13.41 12 19 6.41Z"/></svg>
        </button>
      </nav>
      
    </form>
    <div class="md-search__output">
      <div class="md-search__scrollwrap" data-md-scrollfix>
        <div class="md-search-result" data-md-component="search-result">
          <div class="md-search-result__meta">
            Initializing search
          </div>
          <ol class="md-search-result__list" role="presentation"></ol>
        </div>
      </div>
    </div>
  </div>
</div>
    
    
      <div class="md-header__source">
        <a href="https://github.com/apachecn/pytorch-doc-zh" title="Go to repository" class="md-source" data-md-component="source">
  <div class="md-source__icon md-icon">
    
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 448 512"><!--! Font Awesome Free 6.5.1 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2023 Fonticons, Inc.--><path d="M439.55 236.05 244 40.45a28.87 28.87 0 0 0-40.81 0l-40.66 40.63 51.52 51.52c27.06-9.14 52.68 16.77 43.39 43.68l49.66 49.66c34.23-11.8 61.18 31 35.47 56.69-26.49 26.49-70.21-2.87-56-37.34L240.22 199v121.85c25.3 12.54 22.26 41.85 9.08 55a34.34 34.34 0 0 1-48.55 0c-17.57-17.6-11.07-46.91 11.25-56v-123c-20.8-8.51-24.6-30.74-18.64-45L142.57 101 8.45 235.14a28.86 28.86 0 0 0 0 40.81l195.61 195.6a28.86 28.86 0 0 0 40.8 0l194.69-194.69a28.86 28.86 0 0 0 0-40.81z"/></svg>
  </div>
  <div class="md-source__repository">
    apachecn/pytorch-doc-zh
  </div>
</a>
      </div>
    
  </nav>
  
</header>
    
    <div class="md-container" data-md-component="container">
      
      
        
          
        
      
      <main class="md-main" data-md-component="main">
        <div class="md-main__inner md-grid">
          
            
              
              <div class="md-sidebar md-sidebar--primary" data-md-component="sidebar" data-md-type="navigation" >
                <div class="md-sidebar__scrollwrap">
                  <div class="md-sidebar__inner">
                    



<nav class="md-nav md-nav--primary" aria-label="Navigation" data-md-level="0">
  <label class="md-nav__title" for="__drawer">
    <a href="../../../.." title="【布客】PyTorch 中文翻译" class="md-nav__button md-logo" aria-label="【布客】PyTorch 中文翻译" data-md-component="logo">
      
  <img src="https://data.apachecn.org/img/logo/logo_green.png" alt="logo">

    </a>
    【布客】PyTorch 中文翻译
  </label>
  
    <div class="md-nav__source">
      <a href="https://github.com/apachecn/pytorch-doc-zh" title="Go to repository" class="md-source" data-md-component="source">
  <div class="md-source__icon md-icon">
    
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 448 512"><!--! Font Awesome Free 6.5.1 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2023 Fonticons, Inc.--><path d="M439.55 236.05 244 40.45a28.87 28.87 0 0 0-40.81 0l-40.66 40.63 51.52 51.52c27.06-9.14 52.68 16.77 43.39 43.68l49.66 49.66c34.23-11.8 61.18 31 35.47 56.69-26.49 26.49-70.21-2.87-56-37.34L240.22 199v121.85c25.3 12.54 22.26 41.85 9.08 55a34.34 34.34 0 0 1-48.55 0c-17.57-17.6-11.07-46.91 11.25-56v-123c-20.8-8.51-24.6-30.74-18.64-45L142.57 101 8.45 235.14a28.86 28.86 0 0 0 0 40.81l195.61 195.6a28.86 28.86 0 0 0 40.8 0l194.69-194.69a28.86 28.86 0 0 0 0-40.81z"/></svg>
  </div>
  <div class="md-source__repository">
    apachecn/pytorch-doc-zh
  </div>
</a>
    </div>
  
  <ul class="md-nav__list" data-md-scrollfix>
    
      
      
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../.." class="md-nav__link">
        
  
  <span class="md-ellipsis">
    PyTorch 中文文档 & 教程
  </span>
  

      </a>
    </li>
  

    
      
      
  
  
  
  
    
    
    
    
    <li class="md-nav__item md-nav__item--nested">
      
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_2" >
        
          
          <label class="md-nav__link" for="__nav_2" id="__nav_2_label" tabindex="0">
            
  
  <span class="md-ellipsis">
    PyTorch 新特性
  </span>
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="1" aria-labelledby="__nav_2_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_2">
            <span class="md-nav__icon md-icon"></span>
            PyTorch 新特性
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../../LatestChanges/PyTorch_V2.0/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    V2.0
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../../LatestChanges/PyTorch_V1.13/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    V1.13
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../../LatestChanges/PyTorch_V1.12/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    V1.12
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../../LatestChanges/PyTorch_V1.11/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    V1.11
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../../LatestChanges/PyTorch_V1.10/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    V1.10
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../../LatestChanges/PyTorch_V1.9/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    V1.9
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../../LatestChanges/PyTorch_V1.8/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    V1.8
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../../LatestChanges/PyTorch_V1.7/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    V1.7
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../../LatestChanges/PyTorch_V1.6/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    V1.6
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../../LatestChanges/PyTorch_V1.5/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    V1.5
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../../LatestChanges/PyTorch_V1.4/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    V1.4
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../../LatestChanges/PyTorch_V1.3/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    V1.3
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../../LatestChanges/PyTorch_V1.2/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    V1.2
  </span>
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

    
      
      
  
  
    
  
  
  
    
    
    
    
    <li class="md-nav__item md-nav__item--active md-nav__item--nested">
      
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_3" checked>
        
          
          <label class="md-nav__link" for="__nav_3" id="__nav_3_label" tabindex="0">
            
  
  <span class="md-ellipsis">
    PyTorch 2.x 中文文档 & 教程
  </span>
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="1" aria-labelledby="__nav_3_label" aria-expanded="true">
          <label class="md-nav__title" for="__nav_3">
            <span class="md-nav__icon md-icon"></span>
            PyTorch 2.x 中文文档 & 教程
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
  
    
    
    
    
    <li class="md-nav__item md-nav__item--nested">
      
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_3_1" >
        
          
          <label class="md-nav__link" for="__nav_3_1" id="__nav_3_1_label" tabindex="0">
            
  
  <span class="md-ellipsis">
    中文教程
  </span>
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="2" aria-labelledby="__nav_3_1_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_3_1">
            <span class="md-nav__icon md-icon"></span>
            中文教程
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
  
    
    
    
    
    <li class="md-nav__item md-nav__item--nested">
      
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_3_1_1" >
        
          
          <label class="md-nav__link" for="__nav_3_1_1" id="__nav_3_1_1_label" tabindex="0">
            
  
  <span class="md-ellipsis">
    PyTorch Recipes
  </span>
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="3" aria-labelledby="__nav_3_1_1_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_3_1_1">
            <span class="md-nav__icon md-icon"></span>
            PyTorch Recipes
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../tutorials/recipes/recipes_index/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    See All Recipes
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../tutorials/prototype/prototype_index/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    See All Prototype Recipes
  </span>
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

              
            
              
                
  
  
  
  
    
    
    
    
    <li class="md-nav__item md-nav__item--nested">
      
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_3_1_2" >
        
          
          <label class="md-nav__link" for="__nav_3_1_2" id="__nav_3_1_2_label" tabindex="0">
            
  
  <span class="md-ellipsis">
    Introduction to PyTorch
  </span>
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="3" aria-labelledby="__nav_3_1_2_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_3_1_2">
            <span class="md-nav__icon md-icon"></span>
            Introduction to PyTorch
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../tutorials/beginner/basics/intro/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Learn the Basics
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../tutorials/beginner/basics/quickstart_tutorial/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Quickstart
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../tutorials/beginner/basics/tensorqs_tutorial/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Tensors
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../tutorials/beginner/basics/data_tutorial/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Datasets & DataLoaders
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../tutorials/beginner/basics/transforms_tutorial/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Transforms
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../tutorials/beginner/basics/buildmodel_tutorial/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Build the Neural Network
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../tutorials/beginner/basics/autogradqs_tutorial/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Automatic Differentiation with torch.autograd
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../tutorials/beginner/basics/optimization_tutorial/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Optimizing Model Parameters
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../tutorials/beginner/basics/saveloadrun_tutorial/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Save and Load the Model
  </span>
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

              
            
              
                
  
  
  
  
    
    
    
    
    <li class="md-nav__item md-nav__item--nested">
      
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_3_1_3" >
        
          
          <label class="md-nav__link" for="__nav_3_1_3" id="__nav_3_1_3_label" tabindex="0">
            
  
  <span class="md-ellipsis">
    Introduction to PyTorch on YouTube
  </span>
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="3" aria-labelledby="__nav_3_1_3_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_3_1_3">
            <span class="md-nav__icon md-icon"></span>
            Introduction to PyTorch on YouTube
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../tutorials/beginner/introyt/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Introduction to PyTorch - YouTube Series
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../tutorials/beginner/introyt/introyt1_tutorial/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Introduction to PyTorch
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../tutorials/beginner/introyt/tensors_deeper_tutorial/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Introduction to PyTorch Tensors
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../tutorials/beginner/introyt/autogradyt_tutorial/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    The Fundamentals of Autograd
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../tutorials/beginner/introyt/modelsyt_tutorial/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Building Models with PyTorch
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../tutorials/beginner/introyt/tensorboardyt_tutorial/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    PyTorch TensorBoard Support
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../tutorials/beginner/introyt/trainingyt/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Training with PyTorch
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../tutorials/beginner/introyt/captumyt/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Model Understanding with Captum
  </span>
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

              
            
              
                
  
  
  
  
    
    
    
    
    <li class="md-nav__item md-nav__item--nested">
      
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_3_1_4" >
        
          
          <label class="md-nav__link" for="__nav_3_1_4" id="__nav_3_1_4_label" tabindex="0">
            
  
  <span class="md-ellipsis">
    Learning PyTorch
  </span>
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="3" aria-labelledby="__nav_3_1_4_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_3_1_4">
            <span class="md-nav__icon md-icon"></span>
            Learning PyTorch
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../tutorials/beginner/deep_learning_60min_blitz/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Deep Learning with PyTorch: A 60 Minute Blitz
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../tutorials/beginner/pytorch_with_examples/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Learning PyTorch with Examples
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../tutorials/beginner/nn_tutorial/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    What is torch.nn really?
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../tutorials/intermediate/tensorboard_tutorial/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Visualizing Models, Data, and Training with TensorBoard
  </span>
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

              
            
              
                
  
  
  
  
    
    
    
    
    <li class="md-nav__item md-nav__item--nested">
      
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_3_1_5" >
        
          
          <label class="md-nav__link" for="__nav_3_1_5" id="__nav_3_1_5_label" tabindex="0">
            
  
  <span class="md-ellipsis">
    Image and Video
  </span>
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="3" aria-labelledby="__nav_3_1_5_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_3_1_5">
            <span class="md-nav__icon md-icon"></span>
            Image and Video
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../tutorials/intermediate/torchvision_tutorial/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    TorchVision Object Detection Finetuning Tutorial
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../tutorials/beginner/transfer_learning_tutorial/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Transfer Learning for Computer Vision Tutorial
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../tutorials/beginner/fgsm_tutorial/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Adversarial Example Generation
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../tutorials/beginner/dcgan_faces_tutorial/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    DCGAN Tutorial
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../tutorials/intermediate/spatial_transformer_tutorial/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Spatial Transformer Networks Tutorial
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../tutorials/beginner/vt_tutorial/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Optimizing Vision Transformer Model for Deployment
  </span>
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

              
            
              
                
  
  
  
  
    
    
    
    
    <li class="md-nav__item md-nav__item--nested">
      
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_3_1_6" >
        
          
          <label class="md-nav__link" for="__nav_3_1_6" id="__nav_3_1_6_label" tabindex="0">
            
  
  <span class="md-ellipsis">
    Audio
  </span>
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="3" aria-labelledby="__nav_3_1_6_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_3_1_6">
            <span class="md-nav__icon md-icon"></span>
            Audio
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../tutorials/beginner/audio_io_tutorial/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Audio I/O
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../tutorials/beginner/audio_resampling_tutorial/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Audio Resampling
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../tutorials/beginner/audio_data_augmentation_tutorial/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Audio Data Augmentation
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../tutorials/beginner/audio_feature_extractions_tutorial/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Audio Feature Extractions
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../tutorials/beginner/audio_feature_augmentation_tutorial/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Audio Feature Augmentation
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../tutorials/beginner/audio_datasets_tutorial/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Audio Datasets
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../tutorials/intermediate/speech_recognition_pipeline_tutorial/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Speech Recognition with Wav2Vec2
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../tutorials/intermediate/text_to_speech_with_torchaudio/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Text-to-speech with Tacotron2
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../tutorials/intermediate/forced_alignment_with_torchaudio_tutorial/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Forced Alignment with Wav2Vec2
  </span>
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

              
            
              
                
  
  
  
  
    
    
    
    
    <li class="md-nav__item md-nav__item--nested">
      
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_3_1_7" >
        
          
          <label class="md-nav__link" for="__nav_3_1_7" id="__nav_3_1_7_label" tabindex="0">
            
  
  <span class="md-ellipsis">
    Text
  </span>
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="3" aria-labelledby="__nav_3_1_7_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_3_1_7">
            <span class="md-nav__icon md-icon"></span>
            Text
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../tutorials/beginner/transformer_tutorial/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Language Modeling with nn.Transformer and torchtext
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../tutorials/beginner/bettertransformer_tutorial/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Fast Transformer Inference with Better Transformer
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../tutorials/intermediate/char_rnn_classification_tutorial/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    NLP From Scratch: Classifying Names with a Character-Level RNN
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../tutorials/intermediate/char_rnn_generation_tutorial/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    NLP From Scratch: Generating Names with a Character-Level RNN
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../tutorials/intermediate/seq2seq_translation_tutorial/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    NLP From Scratch: Translation with a Sequence to Sequence Network and Attention
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../tutorials/beginner/text_sentiment_ngrams_tutorial/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Text classification with the torchtext library
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../tutorials/beginner/translation_transformer/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Language Translation with nn.Transformer and torchtext
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../tutorials/beginner/torchtext_custom_dataset_tutorial/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Preprocess custom text dataset using Torchtext
  </span>
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

              
            
              
                
  
  
  
  
    
    
    
    
    <li class="md-nav__item md-nav__item--nested">
      
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_3_1_8" >
        
          
          <label class="md-nav__link" for="__nav_3_1_8" id="__nav_3_1_8_label" tabindex="0">
            
  
  <span class="md-ellipsis">
    Backends
  </span>
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="3" aria-labelledby="__nav_3_1_8_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_3_1_8">
            <span class="md-nav__icon md-icon"></span>
            Backends
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../tutorials/beginner/onnx/intro_onnx/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Introduction to ONNX
  </span>
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

              
            
              
                
  
  
  
  
    
    
    
    
    <li class="md-nav__item md-nav__item--nested">
      
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_3_1_9" >
        
          
          <label class="md-nav__link" for="__nav_3_1_9" id="__nav_3_1_9_label" tabindex="0">
            
  
  <span class="md-ellipsis">
    Reinforcement Learning
  </span>
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="3" aria-labelledby="__nav_3_1_9_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_3_1_9">
            <span class="md-nav__icon md-icon"></span>
            Reinforcement Learning
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../tutorials/intermediate/reinforcement_q_learning/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Reinforcement Learning (DQN) Tutorial
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../tutorials/intermediate/reinforcement_ppo/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Reinforcement Learning (PPO) with TorchRL Tutorial
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../tutorials/intermediate/mario_rl_tutorial/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Train a Mario-playing RL Agent
  </span>
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

              
            
              
                
  
  
  
  
    
    
    
    
    <li class="md-nav__item md-nav__item--nested">
      
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_3_1_10" >
        
          
          <label class="md-nav__link" for="__nav_3_1_10" id="__nav_3_1_10_label" tabindex="0">
            
  
  <span class="md-ellipsis">
    Deploying PyTorch Models in Production
  </span>
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="3" aria-labelledby="__nav_3_1_10_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_3_1_10">
            <span class="md-nav__icon md-icon"></span>
            Deploying PyTorch Models in Production
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../tutorials/beginner/onnx/intro_onnx/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Introduction to ONNX
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../tutorials/intermediate/flask_rest_api_tutorial/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Deploying PyTorch in Python via a REST API with Flask
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../tutorials/beginner/Intro_to_TorchScript_tutorial/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Introduction to TorchScript
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../tutorials/advanced/cpp_export/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Loading a TorchScript Model in C++
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../tutorials/advanced/super_resolution_with_onnxruntime/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    (optional) Exporting a Model from PyTorch to ONNX and Running it using ONNX Runtime
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../tutorials/intermediate/realtime_rpi/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Real Time Inference on Raspberry Pi 4 (30 fps!)
  </span>
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

              
            
              
                
  
  
  
  
    
    
    
    
    <li class="md-nav__item md-nav__item--nested">
      
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_3_1_11" >
        
          
          <label class="md-nav__link" for="__nav_3_1_11" id="__nav_3_1_11_label" tabindex="0">
            
  
  <span class="md-ellipsis">
    Code Transforms with FX
  </span>
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="3" aria-labelledby="__nav_3_1_11_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_3_1_11">
            <span class="md-nav__icon md-icon"></span>
            Code Transforms with FX
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../tutorials/intermediate/fx_conv_bn_fuser/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    (beta) Building a Convolution/Batch Norm fuser in FX
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../tutorials/intermediate/fx_profiling_tutorial/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    (beta) Building a Simple CPU Performance Profiler with FX
  </span>
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

              
            
              
                
  
  
  
  
    
    
    
    
    <li class="md-nav__item md-nav__item--nested">
      
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_3_1_12" >
        
          
          <label class="md-nav__link" for="__nav_3_1_12" id="__nav_3_1_12_label" tabindex="0">
            
  
  <span class="md-ellipsis">
    Frontend APIs
  </span>
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="3" aria-labelledby="__nav_3_1_12_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_3_1_12">
            <span class="md-nav__icon md-icon"></span>
            Frontend APIs
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../tutorials/intermediate/memory_format_tutorial/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    (beta) Channels Last Memory Format in PyTorch
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../tutorials/intermediate/forward_ad_usage/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Forward-mode Automatic Differentiation (Beta)
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../tutorials/intermediate/jacobians_hessians/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Jacobians, Hessians, hvp, vhp, and more: composing function transforms
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../tutorials/intermediate/ensembling/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Model ensembling
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../tutorials/intermediate/per_sample_grads/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Per-sample-gradients
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../tutorials/advanced/cpp_frontend/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Using the PyTorch C++ Frontend
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../tutorials/advanced/torch-script-parallelism/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Dynamic Parallelism in TorchScript
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../tutorials/advanced/cpp_autograd/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Autograd in C++ Frontend
  </span>
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

              
            
              
                
  
  
  
  
    
    
    
    
    <li class="md-nav__item md-nav__item--nested">
      
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_3_1_13" >
        
          
          <label class="md-nav__link" for="__nav_3_1_13" id="__nav_3_1_13_label" tabindex="0">
            
  
  <span class="md-ellipsis">
    Extending PyTorch
  </span>
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="3" aria-labelledby="__nav_3_1_13_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_3_1_13">
            <span class="md-nav__icon md-icon"></span>
            Extending PyTorch
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../tutorials/intermediate/custom_function_double_backward_tutorial/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Double Backward with Custom Functions
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../tutorials/intermediate/custom_function_conv_bn_tutorial/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Fusing Convolution and Batch Norm using Custom Function
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../tutorials/advanced/cpp_extension/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Custom C++ and CUDA Extensions
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../tutorials/advanced/torch_script_custom_ops/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Extending TorchScript with Custom C++ Operators
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../tutorials/advanced/torch_script_custom_classes/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Extending TorchScript with Custom C++ Classes
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../tutorials/advanced/dispatcher/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Registering a Dispatched Operator in C++
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../tutorials/advanced/extend_dispatcher/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Extending dispatcher for a new backend in C++
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../tutorials/advanced/privateuseone/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Facilitating New Backend Integration by PrivateUse1
  </span>
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

              
            
              
                
  
  
  
  
    
    
    
    
    <li class="md-nav__item md-nav__item--nested">
      
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_3_1_14" >
        
          
          <label class="md-nav__link" for="__nav_3_1_14" id="__nav_3_1_14_label" tabindex="0">
            
  
  <span class="md-ellipsis">
    Model Optimization
  </span>
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="3" aria-labelledby="__nav_3_1_14_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_3_1_14">
            <span class="md-nav__icon md-icon"></span>
            Model Optimization
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../tutorials/beginner/profiler/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Profiling your PyTorch Module
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../tutorials/intermediate/tensorboard_profiler_tutorial/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    PyTorch Profiler With TensorBoard
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../tutorials/beginner/hyperparameter_tuning_tutorial/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Hyperparameter tuning with Ray Tune
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../tutorials/beginner/vt_tutorial/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Optimizing Vision Transformer Model for Deployment
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../tutorials/intermediate/parametrizations/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Parametrizations Tutorial
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../tutorials/intermediate/pruning_tutorial/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Pruning Tutorial
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../tutorials/advanced/dynamic_quantization_tutorial/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    (beta) Dynamic Quantization on an LSTM Word Language Model
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../tutorials/intermediate/dynamic_quantization_bert_tutorial/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    (beta) Dynamic Quantization on BERT
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../tutorials/intermediate/quantized_transfer_learning_tutorial/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    (beta) Quantized Transfer Learning for Computer Vision Tutorial
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../tutorials/advanced/static_quantization_tutorial/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    (beta) Static Quantization with Eager Mode in PyTorch
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../tutorials/intermediate/torchserve_with_ipex/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Grokking PyTorch Intel CPU performance from first principles
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../tutorials/intermediate/torchserve_with_ipex_2/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Grokking PyTorch Intel CPU performance from first principles (Part 2)
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../tutorials/intermediate/nvfuser_intro_tutorial/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Getting Started - Accelerate Your Scripts with nvFuser
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../tutorials/intermediate/ax_multiobjective_nas_tutorial/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Multi-Objective NAS with Ax
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../tutorials/intermediate/torch_compile_tutorial/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    torch.compile Tutorial
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../tutorials/intermediate/inductor_debug_cpu/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Inductor CPU backend debugging and profiling
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../tutorials/intermediate/scaled_dot_product_attention_tutorial/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    (Beta) Implementing High-Performance Transformers with Scaled Dot Product Attention (SDPA)
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../tutorials/intermediate/scaled_dot_product_attention_tutorial%23using-sdpa-with-torch-compile/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Using SDPA with torch.compile
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../tutorials/intermediate/scaled_dot_product_attention_tutorial%23conclusion/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Conclusion
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../tutorials/beginner/knowledge_distillation_tutorial/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Knowledge Distillation Tutorial
  </span>
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

              
            
              
                
  
  
  
  
    
    
    
    
    <li class="md-nav__item md-nav__item--nested">
      
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_3_1_15" >
        
          
          <label class="md-nav__link" for="__nav_3_1_15" id="__nav_3_1_15_label" tabindex="0">
            
  
  <span class="md-ellipsis">
    Parallel and Distributed Training
  </span>
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="3" aria-labelledby="__nav_3_1_15_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_3_1_15">
            <span class="md-nav__icon md-icon"></span>
            Parallel and Distributed Training
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../tutorials/distributed/home/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Distributed and Parallel Training Tutorials
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../tutorials/beginner/dist_overview/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    PyTorch Distributed Overview
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../tutorials/beginner/ddp_series_intro/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Distributed Data Parallel in PyTorch - Video Tutorials
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../tutorials/intermediate/model_parallel_tutorial/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Single-Machine Model Parallel Best Practices
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../tutorials/intermediate/ddp_tutorial/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Getting Started with Distributed Data Parallel
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../tutorials/intermediate/dist_tuto/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Writing Distributed Applications with PyTorch
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../tutorials/intermediate/FSDP_tutorial/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Getting Started with Fully Sharded Data Parallel(FSDP)
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../tutorials/intermediate/FSDP_adavnced_tutorial/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Advanced Model Training with Fully Sharded Data Parallel (FSDP)
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../tutorials/intermediate/process_group_cpp_extension_tutorial/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Customize Process Group Backends Using Cpp Extensions
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../tutorials/intermediate/rpc_tutorial/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Getting Started with Distributed RPC Framework
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../tutorials/intermediate/rpc_param_server_tutorial/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Implementing a Parameter Server Using Distributed RPC Framework
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../tutorials/intermediate/dist_pipeline_parallel_tutorial/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Distributed Pipeline Parallelism Using RPC
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../tutorials/intermediate/rpc_async_execution/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Implementing Batch RPC Processing Using Asynchronous Executions
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../tutorials/advanced/rpc_ddp_tutorial/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Combining Distributed DataParallel with Distributed RPC Framework
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../tutorials/intermediate/pipeline_tutorial/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Training Transformer models using Pipeline Parallelism
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../tutorials/advanced/ddp_pipeline/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Training Transformer models using Distributed Data Parallel and Pipeline Parallelism
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../tutorials/advanced/generic_join/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Distributed Training with Uneven Inputs Using the Join Context Manager
  </span>
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

              
            
              
                
  
  
  
  
    
    
    
    
    <li class="md-nav__item md-nav__item--nested">
      
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_3_1_16" >
        
          
          <label class="md-nav__link" for="__nav_3_1_16" id="__nav_3_1_16_label" tabindex="0">
            
  
  <span class="md-ellipsis">
    Mobile
  </span>
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="3" aria-labelledby="__nav_3_1_16_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_3_1_16">
            <span class="md-nav__icon md-icon"></span>
            Mobile
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../tutorials/beginner/deeplabv3_on_ios/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Image Segmentation DeepLabV3 on iOS
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../tutorials/beginner/deeplabv3_on_android/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Image Segmentation DeepLabV3 on Android
  </span>
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

              
            
              
                
  
  
  
  
    
    
    
    
    <li class="md-nav__item md-nav__item--nested">
      
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_3_1_17" >
        
          
          <label class="md-nav__link" for="__nav_3_1_17" id="__nav_3_1_17_label" tabindex="0">
            
  
  <span class="md-ellipsis">
    Recommendation Systems
  </span>
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="3" aria-labelledby="__nav_3_1_17_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_3_1_17">
            <span class="md-nav__icon md-icon"></span>
            Recommendation Systems
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../tutorials/intermediate/torchrec_tutorial/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Introduction to TorchRec
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../tutorials/advanced/sharding/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Exploring TorchRec sharding
  </span>
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

              
            
              
                
  
  
  
  
    
    
    
    
    <li class="md-nav__item md-nav__item--nested">
      
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_3_1_18" >
        
          
          <label class="md-nav__link" for="__nav_3_1_18" id="__nav_3_1_18_label" tabindex="0">
            
  
  <span class="md-ellipsis">
    Multimodality
  </span>
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="3" aria-labelledby="__nav_3_1_18_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_3_1_18">
            <span class="md-nav__icon md-icon"></span>
            Multimodality
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../tutorials/beginner/flava_finetuning_tutorial/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    TorchMultimodal Tutorial: Finetuning FLAVA
  </span>
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

              
            
              
                
  
  
    
  
  
  
    
    
    
    
    <li class="md-nav__item md-nav__item--active md-nav__item--nested">
      
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_3_2" checked>
        
          
          <label class="md-nav__link" for="__nav_3_2" id="__nav_3_2_label" tabindex="0">
            
  
  <span class="md-ellipsis">
    中文文档
  </span>
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="2" aria-labelledby="__nav_3_2_label" aria-expanded="true">
          <label class="md-nav__title" for="__nav_3_2">
            <span class="md-nav__icon md-icon"></span>
            中文文档
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    介绍
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    
    
    
    
    <li class="md-nav__item md-nav__item--nested">
      
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_3_2_2" >
        
          
          <label class="md-nav__link" for="__nav_3_2_2" id="__nav_3_2_2_label" tabindex="0">
            
  
  <span class="md-ellipsis">
    Community
  </span>
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="3" aria-labelledby="__nav_3_2_2_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_3_2_2">
            <span class="md-nav__icon md-icon"></span>
            Community
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../community/build_ci_governance/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    PyTorch Governance | Build + CI
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../community/contribution_guide/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    PyTorch Contribution Guide
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../community/design/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    PyTorch Design Philosophy
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../community/governance/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    PyTorch Governance | Mechanics
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../community/persons_of_interest/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    PyTorch Governance | Maintainers
  </span>
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

              
            
              
                
  
  
    
  
  
  
    
    
    
    
    <li class="md-nav__item md-nav__item--active md-nav__item--nested">
      
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_3_2_3" checked>
        
          
          <label class="md-nav__link" for="__nav_3_2_3" id="__nav_3_2_3_label" tabindex="0">
            
  
  <span class="md-ellipsis">
    Developer Notes
  </span>
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="3" aria-labelledby="__nav_3_2_3_label" aria-expanded="true">
          <label class="md-nav__title" for="__nav_3_2_3">
            <span class="md-nav__icon md-icon"></span>
            Developer Notes
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../amp_examples/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    CUDA Automatic Mixed Precision examples
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../autograd/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Autograd mechanics
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../broadcasting/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Broadcasting semantics
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../cpu_threading_torchscript_inference/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    CPU threading and TorchScript inference
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
    
  
  
  
    <li class="md-nav__item md-nav__item--active">
      
      <input class="md-nav__toggle md-toggle" type="checkbox" id="__toc">
      
      
        
      
      
        <label class="md-nav__link md-nav__link--active" for="__toc">
          
  
  <span class="md-ellipsis">
    CUDA semantics
  </span>
  

          <span class="md-nav__icon md-icon"></span>
        </label>
      
      <a href="./" class="md-nav__link md-nav__link--active">
        
  
  <span class="md-ellipsis">
    CUDA semantics
  </span>
  

      </a>
      
        

<nav class="md-nav md-nav--secondary" aria-label="Table of contents">
  
  
  
    
  
  
    <label class="md-nav__title" for="__toc">
      <span class="md-nav__icon md-icon"></span>
      Table of contents
    </label>
    <ul class="md-nav__list" data-md-component="toc" data-md-scrollfix>
      
        <li class="md-nav__item">
  <a href="#ampere-tensorfloat-32tf32" class="md-nav__link">
    <span class="md-ellipsis">
      Ampere 设备上的 TensorFloat-32(TF32) ¶
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#fp16-gemm-reduced" class="md-nav__link">
    <span class="md-ellipsis">
      FP16 GEMM 中的精度降低 [¶](#reduced
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#bf16-gemm" class="md-nav__link">
    <span class="md-ellipsis">
      BF16 GEMM 中的精度降低 ¶
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#_1" class="md-nav__link">
    <span class="md-ellipsis">
      异步执行 ¶
    </span>
  </a>
  
    <nav class="md-nav" aria-label="异步执行 ¶">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#cuda_1" class="md-nav__link">
    <span class="md-ellipsis">
      CUDA 流 ¶
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#_2" class="md-nav__link">
    <span class="md-ellipsis">
      向后传递的流语义 ¶
    </span>
  </a>
  
    <nav class="md-nav" aria-label="向后传递的流语义 ¶">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#bc-grads" class="md-nav__link">
    <span class="md-ellipsis">
      BC 注意：在默认流上使用 grads ¶
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#_3" class="md-nav__link">
    <span class="md-ellipsis">
      内存管理 ¶
    </span>
  </a>
  
    <nav class="md-nav" aria-label="内存管理 ¶">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#_4" class="md-nav__link">
    <span class="md-ellipsis">
      环境变量 ¶
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#cuda_2" class="md-nav__link">
    <span class="md-ellipsis">
      使用 CUDA 的自定义内存分配器 ¶
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#cublas" class="md-nav__link">
    <span class="md-ellipsis">
      cuBLAS 工作空间 ¶
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#cufft" class="md-nav__link">
    <span class="md-ellipsis">
      cuFFT 计划缓存 ¶
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#_5" class="md-nav__link">
    <span class="md-ellipsis">
      即时编译 ¶
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#_6" class="md-nav__link">
    <span class="md-ellipsis">
      最佳实践 ¶
    </span>
  </a>
  
    <nav class="md-nav" aria-label="最佳实践 ¶">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#_7" class="md-nav__link">
    <span class="md-ellipsis">
      与设备无关的代码 ¶
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#_8" class="md-nav__link">
    <span class="md-ellipsis">
      使用固定内存缓冲区 ¶
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#nnparalleldistributeddataparallel-nndataparallel" class="md-nav__link">
    <span class="md-ellipsis">
      使用 nn.parallel.DistributedDataParallel 而不是多处理或 nn.DataParallel ¶
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#cuda_3" class="md-nav__link">
    <span class="md-ellipsis">
      CUDA 图 ¶
    </span>
  </a>
  
    <nav class="md-nav" aria-label="CUDA 图 ¶">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#cuda_4" class="md-nav__link">
    <span class="md-ellipsis">
      为什么使用 CUDA 图？ ¶
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#pytorch-api" class="md-nav__link">
    <span class="md-ellipsis">
      PyTorch API ¶
    </span>
  </a>
  
    <nav class="md-nav" aria-label="PyTorch API ¶">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#_9" class="md-nav__link">
    <span class="md-ellipsis">
      约束 ¶
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#_10" class="md-nav__link">
    <span class="md-ellipsis">
      非约束 ¶
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#_11" class="md-nav__link">
    <span class="md-ellipsis">
      全网捕获 ¶
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#_12" class="md-nav__link">
    <span class="md-ellipsis">
      部分网络捕获 ¶
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#torchcudaamp" class="md-nav__link">
    <span class="md-ellipsis">
      使用 torch.cuda.amp ¶
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#_13" class="md-nav__link">
    <span class="md-ellipsis">
      与多个流一起使用 ¶
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#distributeddataparallel" class="md-nav__link">
    <span class="md-ellipsis">
      与 DistributedDataParallel 的用法 ¶
    </span>
  </a>
  
    <nav class="md-nav" aria-label="与 DistributedDataParallel 的用法 ¶">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#nccl-296" class="md-nav__link">
    <span class="md-ellipsis">
      NCCL &lt; 2.9.6 ¶
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#nccl-296_1" class="md-nav__link">
    <span class="md-ellipsis">
      NCCL &gt;= 2.9.6 ¶
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#_14" class="md-nav__link">
    <span class="md-ellipsis">
      图形内存管理 ¶
    </span>
  </a>
  
    <nav class="md-nav" aria-label="图形内存管理 ¶">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#_15" class="md-nav__link">
    <span class="md-ellipsis">
      跨捕获共享内存 ¶
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
    </ul>
  
</nav>
      
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../ddp/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Distributed Data Parallel
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../extending/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Extending PyTorch
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../extending.func/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Extending torch.func with autograd.Function
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../faq/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Frequently Asked Questions
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../gradcheck/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Gradcheck mechanics
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../hip/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    HIP (ROCm) semantics
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../large_scale_deployments/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Features for large-scale deployments
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../modules/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Modules
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../mps/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    MPS backend
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../multiprocessing/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Multiprocessing best practices
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../numerical_accuracy/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Numerical accuracy
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../randomness/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Reproducibility
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../serialization/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Serialization semantics
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../windows/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Windows FAQ
  </span>
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

              
            
              
                
  
  
  
  
    
    
    
    
    <li class="md-nav__item md-nav__item--nested">
      
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_3_2_4" >
        
          
          <label class="md-nav__link" for="__nav_3_2_4" id="__nav_3_2_4_label" tabindex="0">
            
  
  <span class="md-ellipsis">
    Language Bindings
  </span>
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="3" aria-labelledby="__nav_3_2_4_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_3_2_4">
            <span class="md-nav__icon md-icon"></span>
            Language Bindings
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../cpp_index/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    C++
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../https:/pytorch.org/javadoc" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Javadoc
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../deploy/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    torch::deploy
  </span>
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

              
            
              
                
  
  
  
  
    
    
    
    
    <li class="md-nav__item md-nav__item--nested">
      
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_3_2_5" >
        
          
          <label class="md-nav__link" for="__nav_3_2_5" id="__nav_3_2_5_label" tabindex="0">
            
  
  <span class="md-ellipsis">
    Python API
  </span>
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="3" aria-labelledby="__nav_3_2_5_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_3_2_5">
            <span class="md-nav__icon md-icon"></span>
            Python API
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../torch/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    torch
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../nn/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    torch.nn
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../nn.functional/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    torch.nn.functional
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../tensors/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    torch.Tensor
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../tensor_attributes/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Tensor Attributes
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../tensor_view/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Tensor Views
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../amp/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    torch.amp
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../autograd/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    torch.autograd
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../library/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    torch.library
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../cpu/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    torch.cpu
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../cuda/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    torch.cuda
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../torch_cuda_memory/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Understanding CUDA Memory Usage
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../torch_cuda_memory#generating-a-snapshot.md" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Generating a Snapshot
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../torch_cuda_memory#using-the-visualizer.md" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Using the visualizer
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../torch_cuda_memory#snapshot-api-reference.md" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Snapshot API Reference
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../mps/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    torch.mps
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../backends/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    torch.backends
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../export/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    torch.export
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../distributed/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    torch.distributed
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../distributed.algorithms.join/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    torch.distributed.algorithms.join
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../distributed.elastic/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    torch.distributed.elastic
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../fsdp/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    torch.distributed.fsdp
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../distributed.optim/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    torch.distributed.optim
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../distributed.tensor.parallel/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    torch.distributed.tensor.parallel
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../distributed.checkpoint/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    torch.distributed.checkpoint
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../distributions/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    torch.distributions
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../torch.compiler/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    torch.compiler
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../fft/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    torch.fft
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../func/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    torch.func
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../futures/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    torch.futures
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../fx/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    torch.fx
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../hub/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    torch.hub
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../jit/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    torch.jit
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../linalg/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    torch.linalg
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../monitor/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    torch.monitor
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../signal/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    torch.signal
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../special/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    torch.special
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../torch.overrides/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    torch.overrides
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../package/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    torch.package
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../profiler/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    torch.profiler
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../nn.init/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    torch.nn.init
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../onnx/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    torch.onnx
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../optim/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    torch.optim
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../complex_numbers/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Complex Numbers
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../ddp_comm_hooks/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    DDP Communication Hooks
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../pipeline/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Pipeline Parallelism
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../quantization/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Quantization
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../rpc/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Distributed RPC Framework
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../random/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    torch.random
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../masked/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    torch.masked
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../nested/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    torch.nested
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../sparse/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    torch.sparse
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../storage/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    torch.Storage
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../testing/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    torch.testing
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../utils/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    torch.utils
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../benchmark_utils/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    torch.utils.benchmark
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../bottleneck/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    torch.utils.bottleneck
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../checkpoint/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    torch.utils.checkpoint
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../cpp_extension/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    torch.utils.cpp_extension
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../data/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    torch.utils.data
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../jit_utils/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    torch.utils.jit
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../dlpack/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    torch.utils.dlpack
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../mobile_optimizer/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    torch.utils.mobile_optimizer
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../model_zoo/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    torch.utils.model_zoo
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../tensorboard/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    torch.utils.tensorboard
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../type_info/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Type Info
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../named_tensor/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Named Tensors
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../name_inference/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Named Tensors operator coverage
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../config_mod/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    torch.__config__
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../logging/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    torch._logging
  </span>
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

              
            
              
                
  
  
  
  
    
    
    
    
    <li class="md-nav__item md-nav__item--nested">
      
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_3_2_6" >
        
          
          <label class="md-nav__link" for="__nav_3_2_6" id="__nav_3_2_6_label" tabindex="0">
            
  
  <span class="md-ellipsis">
    Libraries
  </span>
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="3" aria-labelledby="__nav_3_2_6_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_3_2_6">
            <span class="md-nav__icon md-icon"></span>
            Libraries
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../https:/pytorch.org/audio/stable" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    torchaudio
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../https:/pytorch.org/data" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    TorchData
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../https:/pytorch.org/torchrec" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    TorchRec
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../https:/pytorch.org/serve" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    TorchServe
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../https:/pytorch.org/text/stable" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    torchtext
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../https:/pytorch.org/vision/stable" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    torchvision
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../https:/pytorch.org/xla" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    PyTorch on XLA Devices
  </span>
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

    
      
      
  
  
  
  
    <li class="md-nav__item">
      <a href="https://pytorch1x.apachecn.org" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    PyTorch 1.7 中文文档
  </span>
  

      </a>
    </li>
  

    
      
      
  
  
  
  
    <li class="md-nav__item">
      <a href="https://pytorch1x.apachecn.org" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    PyTorch 1.4 中文文档 & 教程
  </span>
  

      </a>
    </li>
  

    
      
      
  
  
  
  
    <li class="md-nav__item">
      <a href="https://pytorch1x.apachecn.org" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    PyTorch 1.0 中文文档 & 教程
  </span>
  

      </a>
    </li>
  

    
      
      
  
  
  
  
    <li class="md-nav__item">
      <a href="https://pytorch0x.apachecn.org" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    PyTorch 0.4 中文文档
  </span>
  

      </a>
    </li>
  

    
      
      
  
  
  
  
    <li class="md-nav__item">
      <a href="https://pytorch0x.apachecn.org" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    PyTorch 0.3 中文文档 & 教程
  </span>
  

      </a>
    </li>
  

    
      
      
  
  
  
  
    <li class="md-nav__item">
      <a href="https://pytorch0x.apachecn.org" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    PyTorch 0.2 中文文档
  </span>
  

      </a>
    </li>
  

    
      
      
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../../contrib/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    贡献指南
  </span>
  

      </a>
    </li>
  

    
      
      
  
  
  
  
    <li class="md-nav__item">
      <a href="https://www.apachecn.org/about" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    关于我们
  </span>
  

      </a>
    </li>
  

    
      
      
  
  
  
  
    <li class="md-nav__item">
      <a href="https://www.apachecn.org/join" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    加入我们
  </span>
  

      </a>
    </li>
  

    
      
      
  
  
  
  
    <li class="md-nav__item">
      <a href="https://docs.apachecn.org" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    中文资源合集
  </span>
  

      </a>
    </li>
  

    
  </ul>
</nav>
                  </div>
                </div>
              </div>
            
            
              
              <div class="md-sidebar md-sidebar--secondary" data-md-component="sidebar" data-md-type="toc" >
                <div class="md-sidebar__scrollwrap">
                  <div class="md-sidebar__inner">
                    

<nav class="md-nav md-nav--secondary" aria-label="Table of contents">
  
  
  
    
  
  
    <label class="md-nav__title" for="__toc">
      <span class="md-nav__icon md-icon"></span>
      Table of contents
    </label>
    <ul class="md-nav__list" data-md-component="toc" data-md-scrollfix>
      
        <li class="md-nav__item">
  <a href="#ampere-tensorfloat-32tf32" class="md-nav__link">
    <span class="md-ellipsis">
      Ampere 设备上的 TensorFloat-32(TF32) ¶
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#fp16-gemm-reduced" class="md-nav__link">
    <span class="md-ellipsis">
      FP16 GEMM 中的精度降低 [¶](#reduced
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#bf16-gemm" class="md-nav__link">
    <span class="md-ellipsis">
      BF16 GEMM 中的精度降低 ¶
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#_1" class="md-nav__link">
    <span class="md-ellipsis">
      异步执行 ¶
    </span>
  </a>
  
    <nav class="md-nav" aria-label="异步执行 ¶">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#cuda_1" class="md-nav__link">
    <span class="md-ellipsis">
      CUDA 流 ¶
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#_2" class="md-nav__link">
    <span class="md-ellipsis">
      向后传递的流语义 ¶
    </span>
  </a>
  
    <nav class="md-nav" aria-label="向后传递的流语义 ¶">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#bc-grads" class="md-nav__link">
    <span class="md-ellipsis">
      BC 注意：在默认流上使用 grads ¶
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#_3" class="md-nav__link">
    <span class="md-ellipsis">
      内存管理 ¶
    </span>
  </a>
  
    <nav class="md-nav" aria-label="内存管理 ¶">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#_4" class="md-nav__link">
    <span class="md-ellipsis">
      环境变量 ¶
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#cuda_2" class="md-nav__link">
    <span class="md-ellipsis">
      使用 CUDA 的自定义内存分配器 ¶
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#cublas" class="md-nav__link">
    <span class="md-ellipsis">
      cuBLAS 工作空间 ¶
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#cufft" class="md-nav__link">
    <span class="md-ellipsis">
      cuFFT 计划缓存 ¶
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#_5" class="md-nav__link">
    <span class="md-ellipsis">
      即时编译 ¶
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#_6" class="md-nav__link">
    <span class="md-ellipsis">
      最佳实践 ¶
    </span>
  </a>
  
    <nav class="md-nav" aria-label="最佳实践 ¶">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#_7" class="md-nav__link">
    <span class="md-ellipsis">
      与设备无关的代码 ¶
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#_8" class="md-nav__link">
    <span class="md-ellipsis">
      使用固定内存缓冲区 ¶
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#nnparalleldistributeddataparallel-nndataparallel" class="md-nav__link">
    <span class="md-ellipsis">
      使用 nn.parallel.DistributedDataParallel 而不是多处理或 nn.DataParallel ¶
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#cuda_3" class="md-nav__link">
    <span class="md-ellipsis">
      CUDA 图 ¶
    </span>
  </a>
  
    <nav class="md-nav" aria-label="CUDA 图 ¶">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#cuda_4" class="md-nav__link">
    <span class="md-ellipsis">
      为什么使用 CUDA 图？ ¶
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#pytorch-api" class="md-nav__link">
    <span class="md-ellipsis">
      PyTorch API ¶
    </span>
  </a>
  
    <nav class="md-nav" aria-label="PyTorch API ¶">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#_9" class="md-nav__link">
    <span class="md-ellipsis">
      约束 ¶
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#_10" class="md-nav__link">
    <span class="md-ellipsis">
      非约束 ¶
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#_11" class="md-nav__link">
    <span class="md-ellipsis">
      全网捕获 ¶
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#_12" class="md-nav__link">
    <span class="md-ellipsis">
      部分网络捕获 ¶
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#torchcudaamp" class="md-nav__link">
    <span class="md-ellipsis">
      使用 torch.cuda.amp ¶
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#_13" class="md-nav__link">
    <span class="md-ellipsis">
      与多个流一起使用 ¶
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#distributeddataparallel" class="md-nav__link">
    <span class="md-ellipsis">
      与 DistributedDataParallel 的用法 ¶
    </span>
  </a>
  
    <nav class="md-nav" aria-label="与 DistributedDataParallel 的用法 ¶">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#nccl-296" class="md-nav__link">
    <span class="md-ellipsis">
      NCCL &lt; 2.9.6 ¶
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#nccl-296_1" class="md-nav__link">
    <span class="md-ellipsis">
      NCCL &gt;= 2.9.6 ¶
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#_14" class="md-nav__link">
    <span class="md-ellipsis">
      图形内存管理 ¶
    </span>
  </a>
  
    <nav class="md-nav" aria-label="图形内存管理 ¶">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#_15" class="md-nav__link">
    <span class="md-ellipsis">
      跨捕获共享内存 ¶
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
    </ul>
  
</nav>
                  </div>
                </div>
              </div>
            
          
          
            <div class="md-content" data-md-component="content">
              <article class="md-content__inner md-typeset">
                
                  

  
    <a href="https://github.com/apachecn/pytorch-doc-zh/edit/master/docs/2.0/docs/notes/cuda.md" title="Edit this page" class="md-content__button md-icon">
      
      <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M10 20H6V4h7v5h5v3.1l2-2V8l-6-6H6c-1.1 0-2 .9-2 2v16c0 1.1.9 2 2 2h4v-2m10.2-7c.1 0 .3.1.4.2l1.3 1.3c.2.2.2.6 0 .8l-1 1-2.1-2.1 1-1c.1-.1.2-.2.4-.2m0 3.9L14.1 23H12v-2.1l6.1-6.1 2.1 2.1Z"/></svg>
    </a>
  
  
    
      
    
    <a href="https://github.com/apachecn/pytorch-doc-zh/raw/master/docs/2.0/docs/notes/cuda.md" title="View source of this page" class="md-content__button md-icon">
      
      <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M17 18c.56 0 1 .44 1 1s-.44 1-1 1-1-.44-1-1 .44-1 1-1m0-3c-2.73 0-5.06 1.66-6 4 .94 2.34 3.27 4 6 4s5.06-1.66 6-4c-.94-2.34-3.27-4-6-4m0 6.5a2.5 2.5 0 0 1-2.5-2.5 2.5 2.5 0 0 1 2.5-2.5 2.5 2.5 0 0 1 2.5 2.5 2.5 2.5 0 0 1-2.5 2.5M9.27 20H6V4h7v5h5v4.07c.7.08 1.36.25 2 .49V8l-6-6H6a2 2 0 0 0-2 2v16a2 2 0 0 0 2 2h4.5a8.15 8.15 0 0 1-1.23-2Z"/></svg>
    </a>
  


<h1 id="cuda">CUDA 语义 <a href="#cuda-semantics" title="此标题的永久链接">¶</a></h1>
<blockquote>
<p>译者：<a href="https://github.com/jiangzhonglian">片刻小哥哥</a></p>
<p>项目地址：<a href="https://pytorch.apachecn.org/2.0/docs/notes/cuda">https://pytorch.apachecn.org/2.0/docs/notes/cuda</a></p>
<p>原始地址：<a href="https://pytorch.org/docs/stable/notes/cuda.html">https://pytorch.org/docs/stable/notes/cuda.html</a></p>
</blockquote>
<p><a href="../cuda.html#module-torch.cuda" title="torch.cuda"><code>torch.cuda</code></a> 用于设置和运行 CUDA 操作。它会跟踪当前选择的 GPU，并且默认情况下您分配的所有 CUDA tensor都将在该设备上创建。可以使用 <a href="../generated/torch.cuda.device.html#torch.cuda.device" title="torch.cuda.device"><code>torch.cuda.device</code></a> 上下文管理器更改所选设备。</p>
<p>但是，一旦分配了tensor，无论选择什么设备，都可以对其进行操作，并且结果将始终放置在与tensor相同的设备上。</p>
<p>默认情况下不允许跨 GPU 操作，但 <a href="../generated/torch.Tensor.copy_.html#torch.Tensor.copy_" title="torch.Tensor.copy_"><code>copy_()</code></a> 除外以及其他具有类似复制功能的方法，例如 <a href="../generated/torch.Tensor.to.html#torch.Tensor.to" title="torch.Tensor.to"><code>to()</code></a> 和 <a href="../generated/torch.Tensor.cuda.html#torch.Tensor.cuda" title="torch.Tensor.cuda"><code>cuda()</code> </a> 。除非您启用点对点内存访问，否则任何启动操作tensor的尝试都会分布在不同的设备上会引发错误。</p>
<p>下面你可以找到一个展示这一点的小例子：</p>
<div class="highlight"><pre><span></span><code><a id="__codelineno-0-1" name="__codelineno-0-1" href="#__codelineno-0-1"></a>cuda = torch.device(&#39;cuda&#39;)     # Default CUDA device
<a id="__codelineno-0-2" name="__codelineno-0-2" href="#__codelineno-0-2"></a>cuda0 = torch.device(&#39;cuda:0&#39;)
<a id="__codelineno-0-3" name="__codelineno-0-3" href="#__codelineno-0-3"></a>cuda2 = torch.device(&#39;cuda:2&#39;)  # GPU 2 (these are 0-indexed)
<a id="__codelineno-0-4" name="__codelineno-0-4" href="#__codelineno-0-4"></a>
<a id="__codelineno-0-5" name="__codelineno-0-5" href="#__codelineno-0-5"></a>x = torch.tensor([1., 2.], device=cuda0)
<a id="__codelineno-0-6" name="__codelineno-0-6" href="#__codelineno-0-6"></a># x.device is device(type=&#39;cuda&#39;, index=0)
<a id="__codelineno-0-7" name="__codelineno-0-7" href="#__codelineno-0-7"></a>y = torch.tensor([1., 2.]).cuda()
<a id="__codelineno-0-8" name="__codelineno-0-8" href="#__codelineno-0-8"></a># y.device is device(type=&#39;cuda&#39;, index=0)
<a id="__codelineno-0-9" name="__codelineno-0-9" href="#__codelineno-0-9"></a>
<a id="__codelineno-0-10" name="__codelineno-0-10" href="#__codelineno-0-10"></a>with torch.cuda.device(1):
<a id="__codelineno-0-11" name="__codelineno-0-11" href="#__codelineno-0-11"></a>    # allocates a tensor on GPU 1
<a id="__codelineno-0-12" name="__codelineno-0-12" href="#__codelineno-0-12"></a>    a = torch.tensor([1., 2.], device=cuda)
<a id="__codelineno-0-13" name="__codelineno-0-13" href="#__codelineno-0-13"></a>
<a id="__codelineno-0-14" name="__codelineno-0-14" href="#__codelineno-0-14"></a>    # transfers a tensor from CPU to GPU 1
<a id="__codelineno-0-15" name="__codelineno-0-15" href="#__codelineno-0-15"></a>    b = torch.tensor([1., 2.]).cuda()
<a id="__codelineno-0-16" name="__codelineno-0-16" href="#__codelineno-0-16"></a>    # a.device and b.device are device(type=&#39;cuda&#39;, index=1)
<a id="__codelineno-0-17" name="__codelineno-0-17" href="#__codelineno-0-17"></a>
<a id="__codelineno-0-18" name="__codelineno-0-18" href="#__codelineno-0-18"></a>    # You can also use ``Tensor.to`` to transfer a tensor:
<a id="__codelineno-0-19" name="__codelineno-0-19" href="#__codelineno-0-19"></a>    b2 = torch.tensor([1., 2.]).to(device=cuda)
<a id="__codelineno-0-20" name="__codelineno-0-20" href="#__codelineno-0-20"></a>    # b.device and b2.device are device(type=&#39;cuda&#39;, index=1)
<a id="__codelineno-0-21" name="__codelineno-0-21" href="#__codelineno-0-21"></a>
<a id="__codelineno-0-22" name="__codelineno-0-22" href="#__codelineno-0-22"></a>    c = a + b
<a id="__codelineno-0-23" name="__codelineno-0-23" href="#__codelineno-0-23"></a>    # c.device is device(type=&#39;cuda&#39;, index=1)
<a id="__codelineno-0-24" name="__codelineno-0-24" href="#__codelineno-0-24"></a>
<a id="__codelineno-0-25" name="__codelineno-0-25" href="#__codelineno-0-25"></a>    z = x + y
<a id="__codelineno-0-26" name="__codelineno-0-26" href="#__codelineno-0-26"></a>    # z.device is device(type=&#39;cuda&#39;, index=0)
<a id="__codelineno-0-27" name="__codelineno-0-27" href="#__codelineno-0-27"></a>
<a id="__codelineno-0-28" name="__codelineno-0-28" href="#__codelineno-0-28"></a>    # even within a context, you can specify the device
<a id="__codelineno-0-29" name="__codelineno-0-29" href="#__codelineno-0-29"></a>    # (or give a GPU index to the .cuda call)
<a id="__codelineno-0-30" name="__codelineno-0-30" href="#__codelineno-0-30"></a>    d = torch.randn(2, device=cuda2)
<a id="__codelineno-0-31" name="__codelineno-0-31" href="#__codelineno-0-31"></a>    e = torch.randn(2).to(cuda2)
<a id="__codelineno-0-32" name="__codelineno-0-32" href="#__codelineno-0-32"></a>    f = torch.randn(2).cuda(cuda2)
<a id="__codelineno-0-33" name="__codelineno-0-33" href="#__codelineno-0-33"></a>    # d.device, e.device, and f.device are all device(type=&#39;cuda&#39;, index=2)
</code></pre></div>
<h2 id="ampere-tensorfloat-32tf32">Ampere 设备上的 TensorFloat-32(TF32) <a href="#tensorfloat-32-tf32-on-ampere-devices“此标题的永久链接”">¶</a></h2>
<p>从 PyTorch 1.7 开始，有一个名为 allowed_tf32 的新标志。此标志在 PyTorch 1.7 到 PyTorch 1.11 中默认为 True，在 PyTorch 1.12 及更高版本中默认为 False。此标志控制是否允许 PyTorch 使用 TensorFloat32 (TF32) tensor核心，自 Ampere 以来在新的 NVIDIA GPU 上可用，在内部计算 matmul(矩阵)乘法和批量矩阵乘法)和卷积。</p>
<p>TF32 tensor核心旨在通过将输入数据舍入为 10 位尾数，并以 FP32 精度累加结果，从而保持 FP32 动态范围，从而在 torch.float32 tensor上实现 matmul 和卷积方面的更好性能。</p>
<p>matmuls 和卷积是分开控制的，它们相应的标志可以在以下位置访问：</p>
<div class="highlight"><pre><span></span><code><a id="__codelineno-1-1" name="__codelineno-1-1" href="#__codelineno-1-1"></a># The flag below controls whether to allow TF32 on matmul. This flag defaults to False
<a id="__codelineno-1-2" name="__codelineno-1-2" href="#__codelineno-1-2"></a># in PyTorch 1.12 and later.
<a id="__codelineno-1-3" name="__codelineno-1-3" href="#__codelineno-1-3"></a>torch.backends.cuda.matmul.allow_tf32 = True
<a id="__codelineno-1-4" name="__codelineno-1-4" href="#__codelineno-1-4"></a>
<a id="__codelineno-1-5" name="__codelineno-1-5" href="#__codelineno-1-5"></a># The flag below controls whether to allow TF32 on cuDNN. This flag defaults to True.
<a id="__codelineno-1-6" name="__codelineno-1-6" href="#__codelineno-1-6"></a>torch.backends.cudnn.allow_tf32 = True
</code></pre></div>
<p>请注意，除了 matmul 和卷积本身之外，内部使用 matmul 或卷积的函数和 nn 模块也会受到影响。其中包括 nn.Linear 、 nn.Conv* 、cdist、tensordot、仿射网格和网格样本、自适应日志 softmax、GRU 和 LSTM。</p>
<p>要了解精度和速度，请参阅下面的示例代码：</p>
<div class="highlight"><pre><span></span><code><a id="__codelineno-2-1" name="__codelineno-2-1" href="#__codelineno-2-1"></a>a_full = torch.randn(10240, 10240, dtype=torch.double, device=&#39;cuda&#39;)
<a id="__codelineno-2-2" name="__codelineno-2-2" href="#__codelineno-2-2"></a>b_full = torch.randn(10240, 10240, dtype=torch.double, device=&#39;cuda&#39;)
<a id="__codelineno-2-3" name="__codelineno-2-3" href="#__codelineno-2-3"></a>ab_full = a_full @ b_full
<a id="__codelineno-2-4" name="__codelineno-2-4" href="#__codelineno-2-4"></a>mean = ab_full.abs().mean()  # 80.7277
<a id="__codelineno-2-5" name="__codelineno-2-5" href="#__codelineno-2-5"></a>
<a id="__codelineno-2-6" name="__codelineno-2-6" href="#__codelineno-2-6"></a>a = a_full.float()
<a id="__codelineno-2-7" name="__codelineno-2-7" href="#__codelineno-2-7"></a>b = b_full.float()
<a id="__codelineno-2-8" name="__codelineno-2-8" href="#__codelineno-2-8"></a>
<a id="__codelineno-2-9" name="__codelineno-2-9" href="#__codelineno-2-9"></a># Do matmul at TF32 mode.
<a id="__codelineno-2-10" name="__codelineno-2-10" href="#__codelineno-2-10"></a>torch.backends.cuda.matmul.allow_tf32 = True
<a id="__codelineno-2-11" name="__codelineno-2-11" href="#__codelineno-2-11"></a>ab_tf32 = a @ b  # takes 0.016s on GA100
<a id="__codelineno-2-12" name="__codelineno-2-12" href="#__codelineno-2-12"></a>error = (ab_tf32 - ab_full).abs().max()  # 0.1747
<a id="__codelineno-2-13" name="__codelineno-2-13" href="#__codelineno-2-13"></a>relative_error = error / mean  # 0.0022
<a id="__codelineno-2-14" name="__codelineno-2-14" href="#__codelineno-2-14"></a>
<a id="__codelineno-2-15" name="__codelineno-2-15" href="#__codelineno-2-15"></a># Do matmul with TF32 disabled.
<a id="__codelineno-2-16" name="__codelineno-2-16" href="#__codelineno-2-16"></a>torch.backends.cuda.matmul.allow_tf32 = False
<a id="__codelineno-2-17" name="__codelineno-2-17" href="#__codelineno-2-17"></a>ab_fp32 = a @ b  # takes 0.11s on GA100
<a id="__codelineno-2-18" name="__codelineno-2-18" href="#__codelineno-2-18"></a>error = (ab_fp32 - ab_full).abs().max()  # 0.0031
<a id="__codelineno-2-19" name="__codelineno-2-19" href="#__codelineno-2-19"></a>relative_error = error / mean  # 0.000039
</code></pre></div>
<p>从上面的例子中，我们可以看到，启用 TF32 后，速度快了约 7 倍，相对误差与双精度相比大约大 2 个数量级。如果需要完整的 FP32 精度，用户可以通过以下方式禁用 TF32：</p>
<div class="highlight"><pre><span></span><code><a id="__codelineno-3-1" name="__codelineno-3-1" href="#__codelineno-3-1"></a>torch.backends.cuda.matmul.allow_tf32 = False
<a id="__codelineno-3-2" name="__codelineno-3-2" href="#__codelineno-3-2"></a>torch.backends.cudnn.allow_tf32 = False
</code></pre></div>
<p>要在 C++ 中关闭 TF32 标志，您可以执行以下操作</p>
<div class="highlight"><pre><span></span><code><a id="__codelineno-4-1" name="__codelineno-4-1" href="#__codelineno-4-1"></a>at::globalContext().setAllowTF32CuBLAS(false);
<a id="__codelineno-4-2" name="__codelineno-4-2" href="#__codelineno-4-2"></a>at::globalContext().setAllowTF32CuDNN(false);
</code></pre></div>
<p>有关 TF32 的更多信息，请参阅：</p>
<ul>
<li>[TensorFloat-32](https://blogs.nvidia.com/blog/2020/05/14/tensorfloat-32</li>
<li>precision-format/)</li>
<li><a href="https://devblogs.nvidia.com/cuda-11-features-revealed/">CUDA 11</a></li>
<li><a href="https://devblogs.nvidia.com/nvidia-ampere-architecture-in-depth/">Ampere 架构</a></li>
</ul>
<h2 id="fp16-gemm-reduced">FP16 GEMM 中的精度降低 [¶](#reduced</h2>
<ul>
<li>precision-reduction-in-fp16-gemms “此标题的永久链接”)</li>
</ul>
<p>fp16 GEMM 可能会通过一些中间降低的精度降低来完成(例如，在 fp16 而不是 fp32)。这些选择性的精度降低可以在某些工作负载(特别是具有大 k 维的工作负载)和 GPU 架构上实现更高的性能，但代价是数值精度和潜在的溢出。</p>
<p>V100 的一些基准测试数据示例：</p>
<div class="highlight"><pre><span></span><code><a id="__codelineno-5-1" name="__codelineno-5-1" href="#__codelineno-5-1"></a>[--------------------------- bench_gemm_transformer --------------------------]
<a id="__codelineno-5-2" name="__codelineno-5-2" href="#__codelineno-5-2"></a>      [  m ,  k  ,  n  ]    |  allow_fp16_reduc=True  |  allow_fp16_reduc=False
<a id="__codelineno-5-3" name="__codelineno-5-3" href="#__codelineno-5-3"></a>1 threads: --------------------------------------------------------------------
<a id="__codelineno-5-4" name="__codelineno-5-4" href="#__codelineno-5-4"></a>      [4096, 4048, 4096]    |           1634.6        |           1639.8
<a id="__codelineno-5-5" name="__codelineno-5-5" href="#__codelineno-5-5"></a>      [4096, 4056, 4096]    |           1670.8        |           1661.9
<a id="__codelineno-5-6" name="__codelineno-5-6" href="#__codelineno-5-6"></a>      [4096, 4080, 4096]    |           1664.2        |           1658.3
<a id="__codelineno-5-7" name="__codelineno-5-7" href="#__codelineno-5-7"></a>      [4096, 4096, 4096]    |           1639.4        |           1651.0
<a id="__codelineno-5-8" name="__codelineno-5-8" href="#__codelineno-5-8"></a>      [4096, 4104, 4096]    |           1677.4        |           1674.9
<a id="__codelineno-5-9" name="__codelineno-5-9" href="#__codelineno-5-9"></a>      [4096, 4128, 4096]    |           1655.7        |           1646.0
<a id="__codelineno-5-10" name="__codelineno-5-10" href="#__codelineno-5-10"></a>      [4096, 4144, 4096]    |           1796.8        |           2519.6
<a id="__codelineno-5-11" name="__codelineno-5-11" href="#__codelineno-5-11"></a>      [4096, 5096, 4096]    |           2094.6        |           3190.0
<a id="__codelineno-5-12" name="__codelineno-5-12" href="#__codelineno-5-12"></a>      [4096, 5104, 4096]    |           2144.0        |           2663.5
<a id="__codelineno-5-13" name="__codelineno-5-13" href="#__codelineno-5-13"></a>      [4096, 5112, 4096]    |           2149.1        |           2766.9
<a id="__codelineno-5-14" name="__codelineno-5-14" href="#__codelineno-5-14"></a>      [4096, 5120, 4096]    |           2142.8        |           2631.0
<a id="__codelineno-5-15" name="__codelineno-5-15" href="#__codelineno-5-15"></a>      [4096, 9728, 4096]    |           3875.1        |           5779.8
<a id="__codelineno-5-16" name="__codelineno-5-16" href="#__codelineno-5-16"></a>      [4096, 16384, 4096]   |           6182.9        |           9656.5
<a id="__codelineno-5-17" name="__codelineno-5-17" href="#__codelineno-5-17"></a>(times in microseconds).
</code></pre></div>
<p>如果需要完全降低精度，用户可以通过以下方式禁用 fp16 GEMM 中降低的精度：</p>
<div class="highlight"><pre><span></span><code><a id="__codelineno-6-1" name="__codelineno-6-1" href="#__codelineno-6-1"></a>torch.backends.cuda.matmul.allow_fp16_reduced_precision_reduction = False
</code></pre></div>
<p>要切换 C++ 中降低的精度降低标志，可以这样做</p>
<div class="highlight"><pre><span></span><code><a id="__codelineno-7-1" name="__codelineno-7-1" href="#__codelineno-7-1"></a>at::globalContext().setAllowFP16ReductionCuBLAS(false);
</code></pre></div>
<h2 id="bf16-gemm">BF16 GEMM 中的精度降低 <a href="#reduced-precision-reduction-in-bf16-gemms" title="此标题的永久链接">¶</a></h2>
<p>BFloat16 GEMM 存在类似的标志(如上所述)。请注意，对于 BF16，此开关默认设置为 True ，如果您观察到工作负载中的数值不稳定，您可能希望将其设置为 False 。</p>
<p>如果不希望降低精度，用户可以通过以下方式禁用 bf16 GEMM 中的降低精度：</p>
<div class="highlight"><pre><span></span><code><a id="__codelineno-8-1" name="__codelineno-8-1" href="#__codelineno-8-1"></a>torch.backends.cuda.matmul.allow_bf16_reduced_precision_reduction = False
</code></pre></div>
<p>要切换 C++ 中降低的精度降低标志，可以这样做</p>
<div class="highlight"><pre><span></span><code><a id="__codelineno-9-1" name="__codelineno-9-1" href="#__codelineno-9-1"></a>at::globalContext().setAllowBF16ReductionCuBLAS(true);
</code></pre></div>
<h2 id="_1">异步执行 <a href="#asynchronous-execution" title="永久链接到此标题">¶</a></h2>
<p>默认情况下，GPU 操作是异步的。当您调用使用 GPU 的函数时，操作会“排队”到特定设备，但不一定要稍后执行。这使我们能够并行执行更多计算，包括 CPU 或其他 GPU 上的操作。</p>
<p>一般来说，异步计算的效果对调用者来说是不可见的，因为(1)每个设备按照排队的顺序执行操作，(2)PyTorch 在 CPU 和 GPU 之间或两个 GPU 之间复制数据时自动执行必要的同步。因此，如果每个操作都是同步执行的，计算就会继续进行。</p>
<p>您可以通过设置环境变量“CUDA_LAUNCH_BLOCKING=1”来强制同步计算。当 GPU 上发生错误时，这会很方便。(对于异步执行，只有在操作实际执行之后才会报告此类错误，因此堆栈跟踪不会显示请求的位置。)</p>
<p>异步计算的结果是没有同步的时间测量不准确。为了获得精确的测量，应该在测量之前调用 <a href="../generated/torch.cuda.synchronize.html#torch.cuda.synchronize" title="torch.cuda.synchronize"><code>torch.cuda.synchronize()</code></a>，或者使用 <a href="../generated/torch.cuda.Event.html#torch.cuda.Event" title="torch.cuda.Event"><code>torch.cuda.Event</code></a> 记录时间，如下所示：</p>
<div class="highlight"><pre><span></span><code><a id="__codelineno-10-1" name="__codelineno-10-1" href="#__codelineno-10-1"></a>start_event = torch.cuda.Event(enable_timing=True)
<a id="__codelineno-10-2" name="__codelineno-10-2" href="#__codelineno-10-2"></a>end_event = torch.cuda.Event(enable_timing=True)
<a id="__codelineno-10-3" name="__codelineno-10-3" href="#__codelineno-10-3"></a>start_event.record()
<a id="__codelineno-10-4" name="__codelineno-10-4" href="#__codelineno-10-4"></a>
<a id="__codelineno-10-5" name="__codelineno-10-5" href="#__codelineno-10-5"></a># Run some things here
<a id="__codelineno-10-6" name="__codelineno-10-6" href="#__codelineno-10-6"></a>
<a id="__codelineno-10-7" name="__codelineno-10-7" href="#__codelineno-10-7"></a>end_event.record()
<a id="__codelineno-10-8" name="__codelineno-10-8" href="#__codelineno-10-8"></a>torch.cuda.synchronize()  # Wait for the events to be recorded!
<a id="__codelineno-10-9" name="__codelineno-10-9" href="#__codelineno-10-9"></a>elapsed_time_ms = start_event.elapsed_time(end_event)
</code></pre></div>
<p>作为例外，有几个函数，例如 <a href="../generated/torch.Tensor.to.html#torch.Tensor.to" title="torch.Tensor.to"><code>to()</code></a> 和 <a href="../generated/torch.Tensor.copy_.html#torch.Tensor.copy_" title="torch.Tensor.copy_"><code>copy_()</code></a> 承认一个显式的 <code>non_blocking</code> 参数，它允许调用者在不必要时绕过同步。另一个例外是 CUDA 流，如下所述。</p>
<h3 id="cuda_1">CUDA 流 <a href="#cuda-streams" title="此标题的永久链接">¶</a></h3>
<p><a href="https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#streams">CUDA 流</a> 是属于特定设备的线性执行序列。您通常不需要显式创建一个：默认情况下，每个设备都使用自己的“默认”流。</p>
<p>每个流内的操作按照它们创建的顺序进行序列化，但是来自不同流的操作可以以任何相对顺序同时执行，除非显式同步函数(例如 <a href="../generated/torch.cuda.synchronize.html#torch.cuda.synchronize" title="torch.cuda.synchronize"><code>synchronize()</code></a> 或 <a href="../generated/torch.cuda.Stream.html#torch.cuda.Stream.wait_stream" title="torch.使用 cuda.Stream.wait_stream"><code>wait_stream()</code></a> )。例如，下面的代码是不正确的：</p>
<div class="highlight"><pre><span></span><code><a id="__codelineno-11-1" name="__codelineno-11-1" href="#__codelineno-11-1"></a>cuda = torch.device(&#39;cuda&#39;)
<a id="__codelineno-11-2" name="__codelineno-11-2" href="#__codelineno-11-2"></a>s = torch.cuda.Stream()  # Create a new stream.
<a id="__codelineno-11-3" name="__codelineno-11-3" href="#__codelineno-11-3"></a>A = torch.empty((100, 100), device=cuda).normal_(0.0, 1.0)
<a id="__codelineno-11-4" name="__codelineno-11-4" href="#__codelineno-11-4"></a>with torch.cuda.stream(s):
<a id="__codelineno-11-5" name="__codelineno-11-5" href="#__codelineno-11-5"></a>    # sum() may start execution before normal_() finishes!
<a id="__codelineno-11-6" name="__codelineno-11-6" href="#__codelineno-11-6"></a>    B = torch.sum(A)
</code></pre></div>
<p>当“当前流”是默认流时，PyTorch 在数据移动时自动执行必要的同步，如上所述。但是，当使用非默认流时，用户有责任确保正确的同步。</p>
<h3 id="_2">向后传递的流语义 <a href="#stream-semantics-of-backward-passes" title="Permalink to this header">¶</a></h3>
<p>每个向后 CUDA 操作都在用于其相应前向操作的同一流上运行。如果您的前向传递在不同流上并行运行独立操作，这有助于向后传递利用相同的并行性。</p>
<p>相对于周围操作的向后调用的流语义与任何其他调用相同。向后传递会插入内部同步，以确保即使向后操作在多个流上运行(如上一段所述)。更具体地说，当调用 <a href="../generated/torch.autograd.backward.html#torch.autograd.backward" title="torch.autograd.backward"><code>autograd.backward</code></a> 、 <a href="../generated/torch.autograd.grad.html#torch.autograd.grad" title="torch.autograd.grad"><code>autograd.grad</code></a> 或 <a href="../generated/torch.Tensor.backward.html#torch.Tensor.backward" title="torch.Tensor.backward"> <code>tensor.backward</code></a> ，并可选择提供 CUDA tensor作为初始梯度 (例如， <a href="../generated/torch.autograd.backward.html#torch.autograd.backward" title="torch.autograd.backward"><code>autograd.backward(..., grad_tensors=initial_grads)</code></a> ， <a href="../generated/torch.autograd.grad.html#torch.autograd.grad" title="torch.autograd.grad"><code>autograd.grad(..., grad_outputs=initial_grads)</code></a> ，或 <a href="../generated/torch.Tensor.backward.html#torch.Tensor.backward" title="torch.Tensor.backward"> <code>tensor.backward(...,gradient=initial_grad)</code></a> ),的行为</p>
<ol>
<li>可选择填充初始梯度，2.调用向后传递，以及 3．使用渐变</li>
</ol>
<p>与任何一组操作具有相同的流语义关系：</p>
<div class="highlight"><pre><span></span><code><a id="__codelineno-12-1" name="__codelineno-12-1" href="#__codelineno-12-1"></a>s = torch.cuda.Stream()
<a id="__codelineno-12-2" name="__codelineno-12-2" href="#__codelineno-12-2"></a>
<a id="__codelineno-12-3" name="__codelineno-12-3" href="#__codelineno-12-3"></a># Safe, grads are used in the same stream context as backward()
<a id="__codelineno-12-4" name="__codelineno-12-4" href="#__codelineno-12-4"></a>with torch.cuda.stream(s):
<a id="__codelineno-12-5" name="__codelineno-12-5" href="#__codelineno-12-5"></a>    loss.backward()
<a id="__codelineno-12-6" name="__codelineno-12-6" href="#__codelineno-12-6"></a>    use grads
<a id="__codelineno-12-7" name="__codelineno-12-7" href="#__codelineno-12-7"></a>
<a id="__codelineno-12-8" name="__codelineno-12-8" href="#__codelineno-12-8"></a># Unsafe
<a id="__codelineno-12-9" name="__codelineno-12-9" href="#__codelineno-12-9"></a>with torch.cuda.stream(s):
<a id="__codelineno-12-10" name="__codelineno-12-10" href="#__codelineno-12-10"></a>    loss.backward()
<a id="__codelineno-12-11" name="__codelineno-12-11" href="#__codelineno-12-11"></a>use grads
<a id="__codelineno-12-12" name="__codelineno-12-12" href="#__codelineno-12-12"></a>
<a id="__codelineno-12-13" name="__codelineno-12-13" href="#__codelineno-12-13"></a># Safe, with synchronization
<a id="__codelineno-12-14" name="__codelineno-12-14" href="#__codelineno-12-14"></a>with torch.cuda.stream(s):
<a id="__codelineno-12-15" name="__codelineno-12-15" href="#__codelineno-12-15"></a>    loss.backward()
<a id="__codelineno-12-16" name="__codelineno-12-16" href="#__codelineno-12-16"></a>torch.cuda.current_stream().wait_stream(s)
<a id="__codelineno-12-17" name="__codelineno-12-17" href="#__codelineno-12-17"></a>use grads
<a id="__codelineno-12-18" name="__codelineno-12-18" href="#__codelineno-12-18"></a>
<a id="__codelineno-12-19" name="__codelineno-12-19" href="#__codelineno-12-19"></a># Safe, populating initial grad and invoking backward are in the same stream context
<a id="__codelineno-12-20" name="__codelineno-12-20" href="#__codelineno-12-20"></a>with torch.cuda.stream(s):
<a id="__codelineno-12-21" name="__codelineno-12-21" href="#__codelineno-12-21"></a>    loss.backward(gradient=torch.ones_like(loss))
<a id="__codelineno-12-22" name="__codelineno-12-22" href="#__codelineno-12-22"></a>
<a id="__codelineno-12-23" name="__codelineno-12-23" href="#__codelineno-12-23"></a># Unsafe, populating initial_grad and invoking backward are in different stream contexts,
<a id="__codelineno-12-24" name="__codelineno-12-24" href="#__codelineno-12-24"></a># without synchronization
<a id="__codelineno-12-25" name="__codelineno-12-25" href="#__codelineno-12-25"></a>initial_grad = torch.ones_like(loss)
<a id="__codelineno-12-26" name="__codelineno-12-26" href="#__codelineno-12-26"></a>with torch.cuda.stream(s):
<a id="__codelineno-12-27" name="__codelineno-12-27" href="#__codelineno-12-27"></a>    loss.backward(gradient=initial_grad)
<a id="__codelineno-12-28" name="__codelineno-12-28" href="#__codelineno-12-28"></a>
<a id="__codelineno-12-29" name="__codelineno-12-29" href="#__codelineno-12-29"></a># Safe, with synchronization
<a id="__codelineno-12-30" name="__codelineno-12-30" href="#__codelineno-12-30"></a>initial_grad = torch.ones_like(loss)
<a id="__codelineno-12-31" name="__codelineno-12-31" href="#__codelineno-12-31"></a>s.wait_stream(torch.cuda.current_stream())
<a id="__codelineno-12-32" name="__codelineno-12-32" href="#__codelineno-12-32"></a>with torch.cuda.stream(s):
<a id="__codelineno-12-33" name="__codelineno-12-33" href="#__codelineno-12-33"></a>    initial_grad.record_stream(s)
<a id="__codelineno-12-34" name="__codelineno-12-34" href="#__codelineno-12-34"></a>    loss.backward(gradient=initial_grad)
</code></pre></div>
<h4 id="bc-grads">BC 注意：在默认流上使用 grads <a href="#bc-note-using-grads-on-the-default-stream" title="Permalink to this header">¶</a></h4>
<p>在 PyTorch 的早期版本(1.9 及更早版本)中，autograd 引擎始终将默认流与所有向后操作同步，因此以下模式：</p>
<div class="highlight"><pre><span></span><code><a id="__codelineno-13-1" name="__codelineno-13-1" href="#__codelineno-13-1"></a>with torch.cuda.stream(s):
<a id="__codelineno-13-2" name="__codelineno-13-2" href="#__codelineno-13-2"></a>    loss.backward()
<a id="__codelineno-13-3" name="__codelineno-13-3" href="#__codelineno-13-3"></a>use grads
</code></pre></div>
<p>只要“use grads”发生在默认流上，它就是安全的。在目前的 PyTorch 中，该模式不再安全。如果“backward()”和“use grads”位于不同的流上下文中，则必须同步流：</p>
<div class="highlight"><pre><span></span><code><a id="__codelineno-14-1" name="__codelineno-14-1" href="#__codelineno-14-1"></a>with torch.cuda.stream(s):
<a id="__codelineno-14-2" name="__codelineno-14-2" href="#__codelineno-14-2"></a>    loss.backward()
<a id="__codelineno-14-3" name="__codelineno-14-3" href="#__codelineno-14-3"></a>torch.cuda.current_stream().wait_stream(s)
<a id="__codelineno-14-4" name="__codelineno-14-4" href="#__codelineno-14-4"></a>use grads
</code></pre></div>
<p>即使“use grads”位于默认流上。</p>
<h2 id="_3">内存管理 <a href="#memory-management" title="此标题的永久链接">¶</a></h2>
<p>PyTorch 使用缓存内存分配器来加速内存分配。这允许快速内存释放而无需设备同步。但是，分配器管理的未使用内存仍将显示为在“nvidia-smi”中使用。您可以使用 <a href="../generated/torch.cuda.memory_allocated.html#torch.cuda.memory_alulated" title="torch.cuda.memory_alulated"><code>memory_allocated()</code></a> 和 <a href="../generated/torch.cuda.max_memory_alulated.html#torch.cuda.max_memory_alulated" title="torch.cuda.max_memory_alulated"><code>max_memory_allocated()</code> </a> 监视tensor占用的内存，并使用 <a href="../generated /torch.cuda.memory_reserved.html#torch.cuda.memory_reserved" title="torch.cuda.memory_reserved"><code>memory_reserved()</code></a> 和 <a href="../generated/torch.cuda.max_memory_reserved.html#torch. cuda.max_memory_reserved" title="torch.cuda.max_memory_reserved"><code>max_memory_reserved()</code></a> 来监视缓存分配器管理的内存总量。调用 <a href="../generated/torch.cuda.empty_cache.html#torch.cuda.empty_cache" title="torch.cuda.empty_cache"><code>empty_cache()</code></a> 释放 PyTorch 中所有<strong>未使用的</strong>缓存内存，以便这些可以被其他 GPU 应用程序使用。但是，tensor占用的 GPU 内存不会被释放，因此无法增加 PyTorch 可用的 GPU 内存量。</p>
<p>为了更好地了解 CUDA 内存随时间的使用情况，<a href="../torch_cuda_memory.html#torch-cuda-memory">了解 CUDA 内存使用情况</a> 描述了用于捕获和可视化内存使用痕迹的工具。</p>
<p>对于更高级的用户，我们通过 <a href="../generated/torch.cuda.memory_stats.html#torch.cuda.memory_stats" title="torch.cuda.memory_stats"><code>memory_stats()</code></a> 提供更全面的内存基准测试。我们还提供通过 <a href="../generated/torch.cuda.memory_snapshot.html#torch.cuda.memory_snapshot &quot;torch.cuda.memory_snapshot&quot; 捕获内存分配器状态的完整快照的功能"><code>memory_snapshot()</code></a> ，这可以帮助您了解代码生成的底层分配模式。</p>
<h3 id="_4">环境变量 <a href="#environment-variables" title="永久链接到此标题">¶</a></h3>
<p>使用缓存分配器可能会干扰内存检查工具，例如“cuda-memcheck”。要使用“cuda-memcheck”调试内存错误，请在环境中设置“PYTORCH_NO_CUDA_MEMORY_CACHING=1”以禁用缓存。</p>
<p>缓存分配器的行为可以通过环境变量 <code>PYTORCH_CUDA_ALLOC_CONF</code> 进行控制。格式为 <code>PYTORCH_CUDA_ALLOC_CONF=&lt;option&gt;:&lt;value&gt;,&lt;option2&gt;:&lt;value2&gt;...</code> 可用选项：</p>
<ul>
<li><code>backend</code> 允许选择底层分配器实现。 目前，有效的选项是 <code>native</code> ，它使用 PyTorch 的本机实现，以及 <code>cudaMallocAsync</code> ，它使用 <a href="https://developer.nvidia.com/blog/using-cuda-stream-ordered-memory-allocator-part-1/">CUDA 的内置异步分配器</a> 。 <code>cudaMallocAsync</code> 需要 CUDA 11.4 或更高版本。 默认是 <code>native</code> 。 “backend”适用于进程使用的所有设备，并且不能针对每个设备进行指定。</li>
<li><code>max_split_size_mb</code> 防止本机分配器分割大于此大小(以 MB 为单位)的块。 这可以减少碎片，并且可以允许完成一些边界工作负载而不会耗尽内存。 根据分配模式，性能成本可以从“零”到“大量”不等。 默认值是无限的，即所有块都可以拆分。 <a href="../generated/torch.cuda.memory_stats.html#torch.cuda.memory_stats" title="torch.cuda.memory_stats"><code>memory_stats()</code></a> 和 <a href="../generated/torch.cuda.memory_summary.html#torch.cuda.memory_summary" title="torch.cuda.memory_summary"><code>memory_summary()</code></a> 方法对于调整很有用。 对于由于“内存不足”而中止并显示大量非活动拆分块的工作负载，应将此选项用作最后的手段。 <code>max_split_size_mb</code> 仅对 <code>backend:native</code> 有意义。使用 <code>backend:cudaMallocAsync</code> 时，<code>max_split_size_mb</code> 将被忽略。</li>
<li><code>roundup_power2_divisions</code> 有助于将请求的分配大小舍入到最接近的 2 次幂除法并更好地利用块。 在本机 CUDACachingAllocator 中，大小以 512 块大小的倍数向上舍入，因此这对于较小的大小非常有效。 然而，这对于大型附近分配来说可能效率低下，因为每个分配都会分配到不同大小的块，并且这些块的重用被最小化。 这可能会创建大量未使用的块并浪费 GPU 内存容量。 此选项允许将分配大小舍入到最接近的 2 次方除法。 例如，如果我们需要对 1200 的大小进行向上舍入，并且除数为 4，则大小 1200 位于 1024 和 2048 之间，如果我们在它们之间进行 4 次除法，则值为 1024、1280、1536 和 1792。 因此，分配大小 1200 将四舍五入为 1280，作为最接近的 2 次方除法上限。 指定一个值以应用于所有分配大小，或指定一个键值对数组来为每个 2 的幂间隔单独设置 2 次幂除法。 例如，要为 256MB 以下的所有分配设置 1 个分区，为 256MB 和 512MB 之间的分配设置 2 个分区，为 512MB 到 1GB 之间的分配设置 4 个分区，为任何更大的分配设置 8 个分区，请将旋钮值设置为：[256:1,512:2,1024:4,&gt;:8]。 <code>roundup_power2_divisions</code> 仅对 <code>backend:native</code> 有意义。 对于<code>backend:cudaMallocAsync</code>，<code>roundup_power2_divisions</code> 将被忽略。</li>
<li>
<p><code>garbage_collection_threshold</code> 有助于主动回收未使用的 GPU 内存，以避免触发昂贵的同步和回收所有操作 (release_cached_blocks)，这可能不利于延迟关键的 GPU 应用程序(例如服务器)。 设置此阈值(例如 0.8)后，如果 GPU 内存容量使用量超过阈值(即分配给 GPU 应用程序的总内存的 80%)，分配器将开始回收 GPU 内存块。 该算法更喜欢首先释放旧的和未使用的块，以避免释放正在积极重用的块。 阈值应介于大于 0.0 和小于 1.0 之间。 <code>garbage_collection_threshold</code> 仅对 <code>backend:native</code> 有意义。 对于<code>backend:cudaMallocAsync</code>，<code>garbage_collection_threshold</code> 将被忽略。</p>
</li>
<li>
<p><code>expandable_segments</code> (实验性，默认值： False )如果设置为 True ，此设置指示分配器创建 CUDA 分配，这些分配稍后可以扩展以更好地处理作业频繁更改分配大小的情况，例如更改批处理大小。 通常，对于大型 (&gt;2MB) 分配，分配器会调用 cudaMalloc 来获取与用户请求大小相同的分配。 将来，如果这些分配的一部分是空闲的，则可以将其重新用于其他请求。 当程序发出许多大小完全相同或大小甚至是该大小的倍数的请求时，这种方法很有效。 许多深度学习模型都遵循这种行为。 然而，一个常见的例外是批量大小从一次迭代到下一次迭代略有变化，例如 在批量推理中。 当程序最初以批量大小 <code>N</code> 运行时，它将进行适合该大小的分配。如果将来它以大小 <code>N - 1</code> 运行，则现有分配仍然足够大。 但是，如果它以 <code>N + 1</code> 的大小运行，那么它将必须进行稍大的新分配。 并非所有tensor的大小都相同。 有些可能是 <code>(N + 1)*A</code>，其他可能是 <code>(N + 1)*A*B</code>，其中 A 和 B 是模型中的一些非批量维度。 由于分配器会在现有分配足够大时重用现有分配，因此某些数量的 <code>(N + 1)*A</code> 分配实际上会适合已经存在的 <code>N*B*A</code> 段，尽管并不完美。 当模型运行时，它将部分填充所有这些段，在这些段的末尾留下不可用的空闲内存片。 分配器在某些时候需要 cudaMalloc 一个新的 <code>(N + 1)*A*B</code> 段。 如果没有足够的内存，那么现在无法恢复现有段末尾的空闲内存片。 对于 50 层以上深度的模型，此模式可能会重复 50 次以上，从而产生许多条子。</p>
</li>
</ul>
<p>可扩展的段允许分配器最初创建一个段，然后在需要更多内存时扩展其大小。它不是为每个段分配一个段，而是尝试使一个段(每个流)根据需要增长。现在，当 <code>N + 1</code> 情况运行时，分配将很好地平铺到一大段中，直到填满。然后请求更多内存并将其附加到段的末尾。此过程不会创建尽可能多的不可用内存条，因此更有可能成功找到该内存。</p>
<div class="admonition note">
<p class="admonition-title">笔记</p>
<p><a href="../cuda.html#cuda-memory-management-api">CUDA 内存管理 API</a> 报告的一些统计信息特定于 <code>backend:native</code> ，对于 <code>backend:cudaMallocAsync</code> 没有意义。请参阅每个函数的文档字符串了解详细信息。</p>
</div>
<h2 id="cuda_2">使用 CUDA 的自定义内存分配器 <a href="#using-custom-memory-allocators-for-cuda" title="此标题的永久链接">¶</a></h2>
<p>可以将分配器定义为 C/C++ 中的简单函数并将它们编译为共享库，下面的代码显示了一个仅跟踪所有内存操作的基本分配器。</p>
<div class="highlight"><pre><span></span><code><a id="__codelineno-15-1" name="__codelineno-15-1" href="#__codelineno-15-1"></a>#include &lt;sys/types.h&gt;
<a id="__codelineno-15-2" name="__codelineno-15-2" href="#__codelineno-15-2"></a>#include &lt;cuda_runtime_api.h&gt;
<a id="__codelineno-15-3" name="__codelineno-15-3" href="#__codelineno-15-3"></a>#include &lt;iostream&gt;
<a id="__codelineno-15-4" name="__codelineno-15-4" href="#__codelineno-15-4"></a>// Compile with g++ alloc.cc -o alloc.so -I/usr/local/cuda/include -shared -fPIC
<a id="__codelineno-15-5" name="__codelineno-15-5" href="#__codelineno-15-5"></a>extern &quot;C&quot; {
<a id="__codelineno-15-6" name="__codelineno-15-6" href="#__codelineno-15-6"></a>void* my_malloc(ssize_t size, int device, cudaStream_t stream) {
<a id="__codelineno-15-7" name="__codelineno-15-7" href="#__codelineno-15-7"></a> void *ptr;
<a id="__codelineno-15-8" name="__codelineno-15-8" href="#__codelineno-15-8"></a> cudaMalloc(&amp;ptr, size);
<a id="__codelineno-15-9" name="__codelineno-15-9" href="#__codelineno-15-9"></a> std::cout&lt;&lt;&quot;alloc &quot;&lt;&lt;ptr&lt;&lt;size&lt;&lt;std::endl;
<a id="__codelineno-15-10" name="__codelineno-15-10" href="#__codelineno-15-10"></a> return ptr;
<a id="__codelineno-15-11" name="__codelineno-15-11" href="#__codelineno-15-11"></a>}
<a id="__codelineno-15-12" name="__codelineno-15-12" href="#__codelineno-15-12"></a>
<a id="__codelineno-15-13" name="__codelineno-15-13" href="#__codelineno-15-13"></a>void my_free(void* ptr, ssize_t size, int device, cudaStream_t stream) {
<a id="__codelineno-15-14" name="__codelineno-15-14" href="#__codelineno-15-14"></a> std::cout&lt;&lt;&quot;free &quot;&lt;&lt;ptr&lt;&lt; &quot; &quot;&lt;&lt;stream&lt;&lt;std::endl;
<a id="__codelineno-15-15" name="__codelineno-15-15" href="#__codelineno-15-15"></a> cudaFree(ptr);
<a id="__codelineno-15-16" name="__codelineno-15-16" href="#__codelineno-15-16"></a>}
<a id="__codelineno-15-17" name="__codelineno-15-17" href="#__codelineno-15-17"></a>}
</code></pre></div>
<p>这可以通过 <a href="../generated/torch.cuda.CUDAPluggableAllocator.html#torch.cuda.CUDAPluggableAllocator" title="torch.cuda.memory.CUDAPluggableAllocator"><code>torch.cuda.memory.CUDAPluggableAllocator</code></a> 在 python 中使用。用户负责提供.so 文件的路径以及与上面指定的签名匹配的分配/释放函数的名称。</p>
<div class="highlight"><pre><span></span><code><a id="__codelineno-16-1" name="__codelineno-16-1" href="#__codelineno-16-1"></a>import torch
<a id="__codelineno-16-2" name="__codelineno-16-2" href="#__codelineno-16-2"></a>
<a id="__codelineno-16-3" name="__codelineno-16-3" href="#__codelineno-16-3"></a># Load the allocator
<a id="__codelineno-16-4" name="__codelineno-16-4" href="#__codelineno-16-4"></a>new_alloc = torch.cuda.memory.CUDAPluggableAllocator(
<a id="__codelineno-16-5" name="__codelineno-16-5" href="#__codelineno-16-5"></a>    &#39;alloc.so&#39;, &#39;my_malloc&#39;, &#39;my_free&#39;)
<a id="__codelineno-16-6" name="__codelineno-16-6" href="#__codelineno-16-6"></a># Swap the current allocator
<a id="__codelineno-16-7" name="__codelineno-16-7" href="#__codelineno-16-7"></a>torch.cuda.memory.change_current_allocator(new_alloc)
<a id="__codelineno-16-8" name="__codelineno-16-8" href="#__codelineno-16-8"></a># This will allocate memory in the device using the new allocator
<a id="__codelineno-16-9" name="__codelineno-16-9" href="#__codelineno-16-9"></a>b = torch.zeros(10, device=&#39;cuda&#39;)
</code></pre></div>
<div class="highlight"><pre><span></span><code><a id="__codelineno-17-1" name="__codelineno-17-1" href="#__codelineno-17-1"></a>import torch
<a id="__codelineno-17-2" name="__codelineno-17-2" href="#__codelineno-17-2"></a>
<a id="__codelineno-17-3" name="__codelineno-17-3" href="#__codelineno-17-3"></a># Do an initial memory allocator
<a id="__codelineno-17-4" name="__codelineno-17-4" href="#__codelineno-17-4"></a>b = torch.zeros(10, device=&#39;cuda&#39;)
<a id="__codelineno-17-5" name="__codelineno-17-5" href="#__codelineno-17-5"></a># Load the allocator
<a id="__codelineno-17-6" name="__codelineno-17-6" href="#__codelineno-17-6"></a>new_alloc = torch.cuda.memory.CUDAPluggableAllocator(
<a id="__codelineno-17-7" name="__codelineno-17-7" href="#__codelineno-17-7"></a>    &#39;alloc.so&#39;, &#39;my_malloc&#39;, &#39;my_free&#39;)
<a id="__codelineno-17-8" name="__codelineno-17-8" href="#__codelineno-17-8"></a># This will error since the current allocator was already instantiated
<a id="__codelineno-17-9" name="__codelineno-17-9" href="#__codelineno-17-9"></a>torch.cuda.memory.change_current_allocator(new_alloc)
</code></pre></div>
<h2 id="cublas">cuBLAS 工作空间 <a href="#cublas-workspaces" title="此标题的永久链接">¶</a></h2>
<p>对于 cuBLAS 句柄和 CUDA 流的每个组合，如果该句柄和流组合执行需要工作空间的 cuBLAS 内核，则将分配一个 cuBLAS 工作空间。为了避免重复分配工作空间，除非“torch._C”，否则不会释放这些工作空间。调用 <code>_cuda_clearCublasWorkspaces()</code>。每个分配的工作空间大小可以通过环境变量“CUBLAS_WORKSPACE_CONFIG”指定，格式为<code>:[SIZE]:[COUNT]</code>。例如，每个分配的默认工作空间大小为<code>CUBLAS_WORKSPACE_CONFIG=:4096:2:16:8</code> 指定总大小为 <code>2 * 4096 + 8 * 16 KiB</code> 。要强制 cuBLAS 避免使用工作区，请设置 <code>CUBLAS_WORKSPACE_CONFIG=:0:0</code> 。</p>
<h2 id="cufft">cuFFT 计划缓存 <a href="#cufft-plan-cache" title="此标题的永久链接">¶</a></h2>
<p>对于每个 CUDA 设备，cuFFT 计划的 LRU 缓存用于加速重复运行的 FFT 方法(例如，<a href="../generated/torch.fft.fft.html#torch.fft.fft" title="torch.fft.fft"><code>torch.fft.fft()</code></a> ) 在具有相同配置的相同几何形状的 CUDA tensor上。由于某些 cuFFT 计划可能会分配 GPU 内存，因此这些缓存具有最大容量。</p>
<p>您可以通过以下API控制和查询当前设备缓存的属性：</p>
<ul>
<li><code>torch.backends.cuda.cufft_plan_cache.max_size</code> 给出缓存的容量(在 CUDA 10 及更新版本上默认为 4096，在旧 CUDA 版本上默认为 1023)。设置此值会直接修改容量。</li>
<li><code>torch.backends.cuda.cufft_plan_cache.size</code> 给出当前驻留在缓存中的计划数量。</li>
<li><code>torch.backends.cuda.cufft_plan_cache.clear()</code> 清除缓存。</li>
</ul>
<p>要控制和查询非默认设备的计划缓存，您可以使用 <a href="../tensor_attributes.html#torch"><code>torch.device</code></a> 索引 <code>torch.backends.cuda.cufft_plan_cache</code> 对象或设备索引，并访问上述属性之一。例如，要设置设备“1”的缓存容量，可以编写<code>torch.backends.cuda.cufft_plan_cache[1].max_size = 10</code>。</p>
<h2 id="_5">即时编译 <a href="#just-in-time-compilation" title="永久链接到此标题">¶</a></h2>
<p>当在 CUDA tensor上执行时，PyTorch 即时编译一些操作，例如 torch.special.zeta。此编译可能非常耗时(最多几秒钟，具体取决于您的硬件和软件)，并且对于单个运算符来说可能会发生多次，因为许多 PyTorch 运算符实际上从各种内核中进行选择，每个内核都必须编译一次，具体取决于它们的内核输入。此编译每个进程发生一次，或者如果使用内核缓存则仅发生一次。</p>
<p>默认情况下，如果定义了 XDG_CACHE_HOME，PyTorch 在 $XDG_CACHE_HOME/torch/kernels 中创建内核缓存，如果未定义，则在 $HOME/.cache/torch/kernels 中创建内核缓存(Windows 除外，其中没有内核缓存尚未支持)。缓存行为可以通过两个环境变量直接控制。如果 USE_PYTORCH_KERNEL_CACHE 设置为 0，则不会使用缓存，如果设置了 PYTORCH_KERNEL_CACHE_PATH，则该路径将用作内核缓存，而不是默认位置。</p>
<h2 id="_6">最佳实践 <a href="#best-practices" title="此标题的永久链接">¶</a></h2>
<h3 id="_7">与设备无关的代码 <a href="#device-agnostic-code" title="固定链接到此标题">¶</a></h3>
<p>由于 PyTorch 的结构，您可能需要显式编写与设备无关(CPU 或 GPU)的代码；一个例子可能是创建一个新的tensor作为循环神经网络的初始隐藏状态。</p>
<p>第一步是确定是否应该使用 GPU。一种常见的模式是使用 Python 的 argparse 模块读取用户参数，并有一个可用于禁用 CUDA 的标志，与 <a href="../generated/torch.cuda.is_available.html#torch.cuda.is_available" title="torch.cuda.is_available"><code>is_available()</code></a>.在下文中，<code>args.device</code> 生成一个 <a href="../tensor_attributes.html#torch.device" title="torch.device"><code>torch.device</code></a> 对象，可用于将tensor移动到 CPU 或 CUDA。</p>
<div class="highlight"><pre><span></span><code><a id="__codelineno-18-1" name="__codelineno-18-1" href="#__codelineno-18-1"></a>import argparse
<a id="__codelineno-18-2" name="__codelineno-18-2" href="#__codelineno-18-2"></a>import torch
<a id="__codelineno-18-3" name="__codelineno-18-3" href="#__codelineno-18-3"></a>
<a id="__codelineno-18-4" name="__codelineno-18-4" href="#__codelineno-18-4"></a>parser = argparse.ArgumentParser(description=&#39;PyTorch Example&#39;)
<a id="__codelineno-18-5" name="__codelineno-18-5" href="#__codelineno-18-5"></a>parser.add_argument(&#39;--disable-cuda&#39;, action=&#39;store_true&#39;,
<a id="__codelineno-18-6" name="__codelineno-18-6" href="#__codelineno-18-6"></a>                    help=&#39;Disable CUDA&#39;)
<a id="__codelineno-18-7" name="__codelineno-18-7" href="#__codelineno-18-7"></a>args = parser.parse_args()
<a id="__codelineno-18-8" name="__codelineno-18-8" href="#__codelineno-18-8"></a>args.device = None
<a id="__codelineno-18-9" name="__codelineno-18-9" href="#__codelineno-18-9"></a>if not args.disable_cuda and torch.cuda.is_available():
<a id="__codelineno-18-10" name="__codelineno-18-10" href="#__codelineno-18-10"></a>    args.device = torch.device(&#39;cuda&#39;)
<a id="__codelineno-18-11" name="__codelineno-18-11" href="#__codelineno-18-11"></a>else:
<a id="__codelineno-18-12" name="__codelineno-18-12" href="#__codelineno-18-12"></a>    args.device = torch.device(&#39;cpu&#39;)
</code></pre></div>
<div class="admonition note">
<p class="admonition-title">笔记</p>
<p>当评估给定环境中 CUDA 的可用性时( <a href="../generated/torch.cuda.is_available.html#torch.cuda.is_available" title="torch.cuda.is_available"><code>is_available()</code></a> )，PyTorch 的默认行为是调用 CUDA Runtime API 方法 <a href="https://docs.nvidia.com/cuda/cuda-runtime-api/group__CUDART__DEVICE.html#group__CUDART__DEVICE_1g18808e54893cfcaaffeab31a73cc55f">cudaGetDeviceCount</a> 。因为此调用依次初始化 CUDA 驱动程序 API(通过 <a href="https://docs.nvidia.com/cuda/cuda-driver-api/group__CUDA__INITIALIZE.html#group__CUDA__INITIALIZE_1g0a2f1517e1bd8502c7194c3a8c134bc3">cuInit</a> )，如果尚未初始化，则后续的 forks运行 <a href="../generated/torch.cuda.is_available.html#torch.cuda.is_available" title="torch.cuda.is_available"><code>is_available()</code></a> 的进程将失败并出现 CUDA 初始化错误。</p>
</div>
<p>在导入执行 <a href="../generated/torch.cuda.is_available.html#torch.cuda.is_available" title="torch.cuda.is_available"><code>is_available()</code></a> (或在直接执行之前)以便直接 <a href="../generated/torch.cuda.is_available.html#torch.cuda.is_available" title="torch.cuda.is_available"><code>is_available()</code></a> 尝试基于 NVML 的评估( <a href="https://docs.nvidia.com/deploy/nvml-api/group__nvmlDeviceQueries.html#group__nvmlDeviceQueries_1ga93623b195bff04bbe3490ca33c8a42d">nvmlDeviceGetCount_v2</a> )。如果基于 NVML 的评估成功(即 NVML 发现/初始化没有失败)，则 <a href="../generated/torch.cuda.is_available.html#torch.cuda.is_available" title="torch.cuda.is_available"><code>is_available()</code></a> 调用不会毒害后续分叉。</p>
<p>如果 NVML 发现/初始化失败， <a href="../generated/torch.cuda.is_available.html#torch.cuda.is_available" title="torch.cuda.is_available"><code>is_available()</code></a> 将回退到标准 CUDA RuntimeAPI评估和前述的分叉约束将适用。</p>
<p>请注意，上述基于 NVML 的 CUDA 可用性评估提供的保证比默认的 CUDARuntime API 方法(需要 CUDA 初始化才能成功)更弱。在某些情况下，基于 NVML 的检查可能会成功，但随后的 CUDA 初始化会失败。</p>
<p>现在我们有了 <code>args.device</code> ，我们可以使用它在所需的设备上创建一个tensor。</p>
<div class="highlight"><pre><span></span><code><a id="__codelineno-19-1" name="__codelineno-19-1" href="#__codelineno-19-1"></a>x = torch.empty((8, 42), device=args.device)
<a id="__codelineno-19-2" name="__codelineno-19-2" href="#__codelineno-19-2"></a>net = Network().to(device=args.device)
</code></pre></div>
<p>这可以在许多情况下用于生成与设备无关的代码。下面是使用数据加载器的示例：</p>
<div class="highlight"><pre><span></span><code><a id="__codelineno-20-1" name="__codelineno-20-1" href="#__codelineno-20-1"></a>cuda0 = torch.device(&#39;cuda:0&#39;)  # CUDA GPU 0
<a id="__codelineno-20-2" name="__codelineno-20-2" href="#__codelineno-20-2"></a>for i, x in enumerate(train_loader):
<a id="__codelineno-20-3" name="__codelineno-20-3" href="#__codelineno-20-3"></a>    x = x.to(cuda0)
</code></pre></div>
<p>在系统上使用多个 GPU 时，您可以使用“CUDA_VISIBLE_DEVICES”环境标志来管理哪些 GPU 可用于 PyTorch。如上所述，要手动控制在哪个 GPU 上创建tensor，最佳实践是使用 <a href="../generated/torch.cuda.device.html#torch.cuda.device" title="torch.cuda.device"><code>torch.cuda.device</code></a> 上下文管理器。</p>
<div class="highlight"><pre><span></span><code><a id="__codelineno-21-1" name="__codelineno-21-1" href="#__codelineno-21-1"></a>print(&quot;Outside device is 0&quot;)  # On device 0 (default in most scenarios)
<a id="__codelineno-21-2" name="__codelineno-21-2" href="#__codelineno-21-2"></a>with torch.cuda.device(1):
<a id="__codelineno-21-3" name="__codelineno-21-3" href="#__codelineno-21-3"></a>    print(&quot;Inside device is 1&quot;)  # On device 1
<a id="__codelineno-21-4" name="__codelineno-21-4" href="#__codelineno-21-4"></a>print(&quot;Outside device is still 0&quot;)  # On device 0
</code></pre></div>
<p>如果您有一个tensor并且想在同一设备上创建相同类型的新tensor，那么您可以使用 <code>torch.Tensor.new_*</code> 方法(请参阅 <a href="../tensors.html#torch.Tensor" title="torch.Tensor"><code>torch.Tensor</code></a> )。而前面提到的 <code>torch.*</code> 工厂函数( <a href="../torch.html#tensor-creation-ops">Creation Ops</a> ) 取决于当前 GPU 上下文和您传入的属性参数，“torch.Tensor.new_*”方法保留设备和tensor的其他属性。</p>
<p>这是在创建需要在前向传播过程中内部创建新tensor的模块时推荐的做法。</p>
<div class="highlight"><pre><span></span><code><a id="__codelineno-22-1" name="__codelineno-22-1" href="#__codelineno-22-1"></a>cuda = torch.device(&#39;cuda&#39;)
<a id="__codelineno-22-2" name="__codelineno-22-2" href="#__codelineno-22-2"></a>x_cpu = torch.empty(2)
<a id="__codelineno-22-3" name="__codelineno-22-3" href="#__codelineno-22-3"></a>x_gpu = torch.empty(2, device=cuda)
<a id="__codelineno-22-4" name="__codelineno-22-4" href="#__codelineno-22-4"></a>x_cpu_long = torch.empty(2, dtype=torch.int64)
<a id="__codelineno-22-5" name="__codelineno-22-5" href="#__codelineno-22-5"></a>
<a id="__codelineno-22-6" name="__codelineno-22-6" href="#__codelineno-22-6"></a>y_cpu = x_cpu.new_full([3, 2], fill_value=0.3)
<a id="__codelineno-22-7" name="__codelineno-22-7" href="#__codelineno-22-7"></a>print(y_cpu)
<a id="__codelineno-22-8" name="__codelineno-22-8" href="#__codelineno-22-8"></a>
<a id="__codelineno-22-9" name="__codelineno-22-9" href="#__codelineno-22-9"></a>    tensor([[ 0.3000,  0.3000],
<a id="__codelineno-22-10" name="__codelineno-22-10" href="#__codelineno-22-10"></a>            [ 0.3000,  0.3000],
<a id="__codelineno-22-11" name="__codelineno-22-11" href="#__codelineno-22-11"></a>            [ 0.3000,  0.3000]])
<a id="__codelineno-22-12" name="__codelineno-22-12" href="#__codelineno-22-12"></a>
<a id="__codelineno-22-13" name="__codelineno-22-13" href="#__codelineno-22-13"></a>y_gpu = x_gpu.new_full([3, 2], fill_value=-5)
<a id="__codelineno-22-14" name="__codelineno-22-14" href="#__codelineno-22-14"></a>print(y_gpu)
<a id="__codelineno-22-15" name="__codelineno-22-15" href="#__codelineno-22-15"></a>
<a id="__codelineno-22-16" name="__codelineno-22-16" href="#__codelineno-22-16"></a>    tensor([[-5.0000, -5.0000],
<a id="__codelineno-22-17" name="__codelineno-22-17" href="#__codelineno-22-17"></a>            [-5.0000, -5.0000],
<a id="__codelineno-22-18" name="__codelineno-22-18" href="#__codelineno-22-18"></a>            [-5.0000, -5.0000]], device=&#39;cuda:0&#39;)
<a id="__codelineno-22-19" name="__codelineno-22-19" href="#__codelineno-22-19"></a>
<a id="__codelineno-22-20" name="__codelineno-22-20" href="#__codelineno-22-20"></a>y_cpu_long = x_cpu_long.new_tensor([[1, 2, 3]])
<a id="__codelineno-22-21" name="__codelineno-22-21" href="#__codelineno-22-21"></a>print(y_cpu_long)
<a id="__codelineno-22-22" name="__codelineno-22-22" href="#__codelineno-22-22"></a>
<a id="__codelineno-22-23" name="__codelineno-22-23" href="#__codelineno-22-23"></a>    tensor([[ 1,  2,  3]])
</code></pre></div>
<p>如果要创建与另一个tensor相同类型和大小的tensor，并用 1 或 0 填充它， <a href="../generated/torch.ones_like.html#torch.ones_like" title="torch.ones_like"><code>ones_like()</code></a> 或 <a href="../generated/torch.zeros_like.html#torch.zeros_like" title="torch.zeros_like"><code>zeros_like()</code></a> 作为方便的辅助函数提供(也保留 <a href="../tensor_attributes.html#torch.device" title="torch.device"><code>torch.devicetensor的</code></a> 和 <a href="../tensor_attributes.html#torch.dtype" title="torch.dtype"><code>torch.dtype</code></a>。</p>
<div class="highlight"><pre><span></span><code><a id="__codelineno-23-1" name="__codelineno-23-1" href="#__codelineno-23-1"></a>x_cpu = torch.empty(2, 3)
<a id="__codelineno-23-2" name="__codelineno-23-2" href="#__codelineno-23-2"></a>x_gpu = torch.empty(2, 3)
<a id="__codelineno-23-3" name="__codelineno-23-3" href="#__codelineno-23-3"></a>
<a id="__codelineno-23-4" name="__codelineno-23-4" href="#__codelineno-23-4"></a>y_cpu = torch.ones_like(x_cpu)
<a id="__codelineno-23-5" name="__codelineno-23-5" href="#__codelineno-23-5"></a>y_gpu = torch.zeros_like(x_gpu)
</code></pre></div>
<h3 id="_8">使用固定内存缓冲区 <a href="#use-pinned-memory-buffers" title="永久链接到此标题">¶</a></h3>
<div class="admonition warning">
<p class="admonition-title">警告</p>
<p>这是一个高级提示。如果过度使用固定内存，则在 RAM 不足时可能会导致严重问题，并且您应该意识到固定通常是一项昂贵的操作。</p>
</div>
<p>当主机到 GPU 的副本源自固定(页面锁定)内存时，速度要快得多。 CPU tensor和存储公开了一个 <a href="../generated/torch.Tensor.pin_memory.html#torch.Tensor.pin_memory" title="torch.Tensor.pin_memory"><code>pin_memory()</code></a> 方法，该方法返回对象，数据放入固定区域。</p>
<p>此外，一旦固定tensor或存储，您就可以使用异步 GPU 副本。只需将额外的 <code>non_blocking=True</code> 参数传递给 <a href="../generated/torch.Tensor.to.html#torch.Tensor.to" title="torch.Tensor.to"><code>to()</code></a> 或 <a href="../generated/torch.Tensor.cuda.html#torch.Tensor.cuda" title="torch.Tensor.cuda"><code>cuda()</code></a> 称呼。这可用于将数据传输与计算重叠。</p>
<p>您可以通过将 <code>pin_memory=True</code> 传递给 <a href="../data.html#torch.utils.data.DataLoader" title="torch.utils.data.DataLoader"><code>DataLoader</code></a> 返回放置在固定内存中的批次构造函数。</p>
<h3 id="nnparalleldistributeddataparallel-nndataparallel">使用 nn.parallel.DistributedDataParallel 而不是多处理或 nn.DataParallel <a href="#use-nn-parallel-distributeddataparallel-instead-of-multiprocessing-or-nn-dataparallel" title="永久链接到此标题">¶</a></h3>
<p>大多数涉及批量输入和多个 GPU 的用例应默认使用 <a href="../generated/torch.nn.parallel.DistributedDataParallel.html#torch.nn.parallel.DistributedDataParallel" title="torch.nn.parallel.DistributedDataParallel"><code>DistributedDataParallel</code></a>使用多个 GPU。</p>
<p>将 CUDA 模型与 <a href="../multiprocessing.html#module-torch.multiprocessing" title="torch.multiprocessing"><code>multiprocessing</code></a> 一起使用有一些重要的注意事项；除非注意完全满足数据处理要求，否则您的程序可能会出现不正确或未定义的行为。</p>
<p>建议使用 <a href="../generated/torch.nn.parallel.DistributedDataParallel.html#torch.nn.parallel.DistributedDataParallel" title="torch.nn.parallel.DistributedDataParallel"><code>DistributedDataParallel</code></a> ，而不是 <a href="../generated/torch.nn.DataParallel.html#torch.nn.DataParallel" title="torch.nn.DataParallel"><code>DataParallel</code> </a> 进行多 GPU 训练，即使只有一个节点。</p>
<p><a href="../generated/torch.nn.parallel.DistributedDataParallel.html#torch.nn.parallel.DistributedDataParallel" title="torch.nn.parallel.DistributedDataParallel"><code>DistributedDataParallel</code></a> 和 <a href="../generated/torch.nn.DataParallel.html#torch.nn.DataParallel" title="torch.nn.DataParallel"><code>DataParallel</code></a> 是: <a href="../generated/torch.nn.parallel.DistributedDataParallel.html#torch.nn.parallel.DistributedDataParallel" title="torch.nn.parallel.DistributedDataParallel"><code>DistributedDataParallel</code></a> 使用多处理，为每个 GPU 创建一个进程，而 <a href="../generated/torch.nn.DataParallel.html#torch.nn.DataParallel “torch.nn.DataParallel”"><code>DataParallel</code></a>使用多线程。通过使用多处理，每个GPU都有其专用的进程，这避免了Python解释器的GIL带来的性能开销。</p>
<p>如果您使用 <a href="../generated/torch.nn.parallel.DistributedDataParallel.html#torch.nn.parallel.DistributedDataParallel" title="torch.nn.parallel.DistributedDataParallel"><code>DistributedDataParallel</code></a> ，您可以使用 torch.distributed.launch用于启动程序的实用程序，请参阅<a href="../distributed.html#distributed-launch">第三方后端</a> 。</p>
<h2 id="cuda_3">CUDA 图 <a href="#cuda-graphs“此标题的永久链接”">¶</a></h2>
<p>CUDA 图是 aCUDA 流及其依赖流执行的工作(主要是内核及其参数)的记录。有关底层 CUDA API 的一般原理和详细信息，请参阅<a href="https://developer.nvidia.com/blog/cuda-graphs/">CUDA 图入门</a>和 CUDA 的<a href="https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#cuda-graphs">图形部分</a> C 编程指南。</p>
<p>PyTorch 支持使用流捕获构建 CUDA 图(https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#creating-a-graph-using-stream-capture) ，它将 aCUDA 流置于 *捕获模式
* 。发送到捕获流的 CUDA 工作实际上并不在 GPU 上运行。相反，工作被记录在图表中。</p>
<p>捕获后，可以“启动”图表以根据需要多次运行 GPU 工作。每次重播都使用相同的参数运行相同的内核。对于指针参数，这意味着使用相同的内存地址。通过在每次重播之前用新数据(例如，来自新批次)填充输入内存，您可以对新数据重新运行相同的工作。</p>
<h3 id="cuda_4">为什么使用 CUDA 图？ <a href="#why-cuda-graphs“此标题的永久链接”">¶</a></h3>
<p>重放图牺牲了典型急切执行的动态灵活性，以换取<strong>大大减少的CPU开销</strong>。图的参数和内核是固定的，因此图重播会跳过参数设置和内核分派的所有层，包括 Python、C++ 和 CUDA 驱动程序开销。在底层，重放只需调用一次 <a href="https://docs.nvidia.com/cuda/cuda-runtime-api/group__CUDART__GRAPH.html#group__CUDART__GRAPH_1g1accfe1da0c605a577c22d9751a09597">cudaGraphLaunch</a> 即可将整个图的工作提交给 GPU。重放中的内核在 GPU 上的执行速度也稍快，但消除 CPU 开销是主要好处。</p>
<p>如果您的网络的全部或部分是图形安全的(通常这意味着静态形状和静态控制流，但请参阅其他<a href="#capture-constraints">约束</a>)并且您怀疑其运行时间至少在某种程度上是CPU，您应该尝试CUDA图形-有限的。</p>
<h3 id="pytorch-api">PyTorch API <a href="#pytorch-api“此标题的永久链接”">¶</a></h3>
<div class="admonition warning">
<p class="admonition-title">警告</p>
<p>此 API 处于测试阶段，可能会在未来版本中发生变化。</p>
</div>
<p>PyTorch 通过原始的 <a href="../generated/torch.cuda.CUDAGraph.html#torch.cuda.CUDAGraph" title="torch.cuda.CUDAGraph"><code>torch.cuda.CUDAGraph</code></a> 类和两个方便的包装器 <a href="../generated/torch.cuda.graph.html#torch.cuda.graph" title="torch.cuda.graph"><code>torch.cuda.graph</code></a> 和 <a href="../generated/torch.cuda.make_graphed_callables.html#torch.cuda.make_graphed_callables" title="torch.cuda.make_graphed_callables"><code>torch.cuda.make_graphed_callables</code></a>。</p>
<p><a href="../generated/torch.cuda.graph.html#torch.cuda.graph" title="torch.cuda.graph"><code>torch.cuda.graph</code></a> 是一个简单、多功能的上下文管理器，可以在其上下文中捕获 CUDA 工作。在捕获之前，通过运行一些急切的迭代来预热要捕获的工作负载。预热必须发生在侧流上。由于图形在每次重播中读取和写入相同的内存地址，因此您必须维护对在捕获期间保存输入和输出数据的tensor的长期引用。要在新输入数据上运行图形，请复制将新数据添加到捕获的输入tensor，重播图形，然后从捕获的输出tensor读取新输出。示例：</p>
<div class="highlight"><pre><span></span><code><a id="__codelineno-24-1" name="__codelineno-24-1" href="#__codelineno-24-1"></a>g = torch.cuda.CUDAGraph()
<a id="__codelineno-24-2" name="__codelineno-24-2" href="#__codelineno-24-2"></a>
<a id="__codelineno-24-3" name="__codelineno-24-3" href="#__codelineno-24-3"></a># Placeholder input used for capture
<a id="__codelineno-24-4" name="__codelineno-24-4" href="#__codelineno-24-4"></a>static_input = torch.empty((5,), device=&quot;cuda&quot;)
<a id="__codelineno-24-5" name="__codelineno-24-5" href="#__codelineno-24-5"></a>
<a id="__codelineno-24-6" name="__codelineno-24-6" href="#__codelineno-24-6"></a># Warmup before capture
<a id="__codelineno-24-7" name="__codelineno-24-7" href="#__codelineno-24-7"></a>s = torch.cuda.Stream()
<a id="__codelineno-24-8" name="__codelineno-24-8" href="#__codelineno-24-8"></a>s.wait_stream(torch.cuda.current_stream())
<a id="__codelineno-24-9" name="__codelineno-24-9" href="#__codelineno-24-9"></a>with torch.cuda.stream(s):
<a id="__codelineno-24-10" name="__codelineno-24-10" href="#__codelineno-24-10"></a>    for _ in range(3):
<a id="__codelineno-24-11" name="__codelineno-24-11" href="#__codelineno-24-11"></a>        static_output = static_input * 2
<a id="__codelineno-24-12" name="__codelineno-24-12" href="#__codelineno-24-12"></a>torch.cuda.current_stream().wait_stream(s)
<a id="__codelineno-24-13" name="__codelineno-24-13" href="#__codelineno-24-13"></a>
<a id="__codelineno-24-14" name="__codelineno-24-14" href="#__codelineno-24-14"></a># Captures the graph
<a id="__codelineno-24-15" name="__codelineno-24-15" href="#__codelineno-24-15"></a># To allow capture, automatically sets a side stream as the current stream in the context
<a id="__codelineno-24-16" name="__codelineno-24-16" href="#__codelineno-24-16"></a>with torch.cuda.graph(g):
<a id="__codelineno-24-17" name="__codelineno-24-17" href="#__codelineno-24-17"></a>    static_output = static_input * 2
<a id="__codelineno-24-18" name="__codelineno-24-18" href="#__codelineno-24-18"></a>
<a id="__codelineno-24-19" name="__codelineno-24-19" href="#__codelineno-24-19"></a># Fills the graph&#39;s input memory with new data to compute on
<a id="__codelineno-24-20" name="__codelineno-24-20" href="#__codelineno-24-20"></a>static_input.copy_(torch.full((5,), 3, device=&quot;cuda&quot;))
<a id="__codelineno-24-21" name="__codelineno-24-21" href="#__codelineno-24-21"></a>g.replay()
<a id="__codelineno-24-22" name="__codelineno-24-22" href="#__codelineno-24-22"></a># static_output holds the results
<a id="__codelineno-24-23" name="__codelineno-24-23" href="#__codelineno-24-23"></a>print(static_output)  # full of 3 * 2 = 6
<a id="__codelineno-24-24" name="__codelineno-24-24" href="#__codelineno-24-24"></a>
<a id="__codelineno-24-25" name="__codelineno-24-25" href="#__codelineno-24-25"></a># Fills the graph&#39;s input memory with more data to compute on
<a id="__codelineno-24-26" name="__codelineno-24-26" href="#__codelineno-24-26"></a>static_input.copy_(torch.full((5,), 4, device=&quot;cuda&quot;))
<a id="__codelineno-24-27" name="__codelineno-24-27" href="#__codelineno-24-27"></a>g.replay()
<a id="__codelineno-24-28" name="__codelineno-24-28" href="#__codelineno-24-28"></a>print(static_output)  # full of 4 * 2 = 8
</code></pre></div>
<p>请参阅<a href="#whole-network-capture">全网络捕获</a>、<a href="#graphs-with-amp">与 torch.cuda.amp 一起使用</a> 和<a href="#multistream-capture">与多个流一起使用</a> 以了解实际情况和高级模式。</p>
<p><a href="../generated/torch.cuda.make_graphed_callables.html#torch.cuda.make_graphed_callables" title="torch.cuda.make_graphed_callables"><code>make_graphed_callables</code></a> 更复杂。 <a href="../generated/torch.cuda.make_graphed_callables.html#torch.cuda.make_graphed_callables" title="torch.cuda.make_graphed_callables"><code>make_graphed_callables</code></a> 接受 Python 函数和 <a href="../generated/torch.nn.Module.html#torch.nn.Module“torch.nn.Module”"><code>torch.nn.Module</code></a>s。对于每个传递的函数或模块，它都会创建前向传递和反向传递工作的单独图表。请参阅<a href="#partial-network-capture">部分网络捕获</a>。</p>
<h4 id="_9">约束 <a href="#constraints" title="此标题的永久链接">¶</a></h4>
<p>如果一组操作不违反以下任何约束，则它是“可捕获的”。</p>
<p>约束适用于 <a href="../generated/torch.cuda.graph.html#torch.cuda.graph" title="torch.cuda.graph"><code>torch.cuda.graph</code></a> 上下文中的所有工作以及转发中的所有工作以及传递给 <a href="../generated/torch.cuda.make_graphed_callables.html#torch.cuda.make_graphed_callables" title="torch.cuda.make_graphed_callables"><code>torch.cuda.make_graphed_callables()</code></a> 的任何可调用对象的向后传递。</p>
<p>违反任何这些都可能会导致运行时错误：</p>
<ul>
<li>捕获必须发生在非默认流上。 (如果您使用原始的 <a href="../generated/torch.cuda.CUDAGraph.html#torch.cuda.CUDAGraph.capture_begin" title="torch.cuda.CUDAGraph.capture_begin&quot;，这只是一个问题) 和 [CUDAGraph.capture_end](../generated/torch.cuda.CUDAGraph.html#torch.cuda.CUDAGraph.capture_end &quot;torch.cuda.CUDAGraph.capture_end"><code>CUDAGraph.capture_begin</code></a> 调用。 <a href="../generated/torch.cuda.graph.html#torch.cuda.graph" title="torch.cuda.graph"><code>graph</code></a> 和 <a href="../generated/torch.cuda.make_graphed_callables. html#torch.cuda.make_graphed_callables" title="torch.cuda.make_graphed_callables"><code>make_graphed_callables()</code></a> 为您设置一个侧流。)</li>
<li>禁止将 CPU 与 GPU 同步的操作(例如 <code>.item()</code> 调用)。</li>
<li>CUDA RNG 操作允许，但必须使用默认生成器。例如，禁止显式构造新的 <a href="../generated/torch.Generator.html#torch.Generator" title="torch.Generator"><code>torch.Generator</code></a> 实例并将其作为 <code>generator</code> 参数传递给 RNG 函数。</li>
</ul>
<p>违反任何这些都可能会导致无提示的数字错误或未定义的行为：</p>
<ul>
<li>在一个进程内，一次只能进行一次捕获。</li>
<li>当捕获正在进行时，任何未捕获的 CUDA 工作都不能在此进程中运行(在任何线程上)。</li>
<li>不会捕获 CPU 工作。如果捕获的操作包括 CPU 工作，则该工作将在重播期间被忽略。</li>
<li>每个重播都会读取和写入相同的(虚拟)内存地址。</li>
<li>禁止动态控制流(基于 CPU 或 GPU 数据)。</li>
<li>动态形状被禁止。该图假设捕获的操作序列中的每个tensor在每次重播中都具有相同的大小和布局。</li>
<li>允许在捕获中使用多个流，但有<a href="#multistream-capture">限制</a>。</li>
</ul>
<h4 id="_10">非约束 <a href="#non-constraints" title="此标题的永久链接">¶</a></h4>
<ul>
<li>捕获后，图表可以在任何流上重播。</li>
</ul>
<h3 id="_11">全网捕获 <a href="#whole-network-capture" title="永久链接到此标题">¶</a></h3>
<p>如果您的整个网络都是可捕获的，您可以捕获并重放整个迭代：</p>
<div class="highlight"><pre><span></span><code><a id="__codelineno-25-1" name="__codelineno-25-1" href="#__codelineno-25-1"></a>N, D_in, H, D_out = 640, 4096, 2048, 1024
<a id="__codelineno-25-2" name="__codelineno-25-2" href="#__codelineno-25-2"></a>model = torch.nn.Sequential(torch.nn.Linear(D_in, H),
<a id="__codelineno-25-3" name="__codelineno-25-3" href="#__codelineno-25-3"></a>                            torch.nn.Dropout(p=0.2),
<a id="__codelineno-25-4" name="__codelineno-25-4" href="#__codelineno-25-4"></a>                            torch.nn.Linear(H, D_out),
<a id="__codelineno-25-5" name="__codelineno-25-5" href="#__codelineno-25-5"></a>                            torch.nn.Dropout(p=0.1)).cuda()
<a id="__codelineno-25-6" name="__codelineno-25-6" href="#__codelineno-25-6"></a>loss_fn = torch.nn.MSELoss()
<a id="__codelineno-25-7" name="__codelineno-25-7" href="#__codelineno-25-7"></a>optimizer = torch.optim.SGD(model.parameters(), lr=0.1)
<a id="__codelineno-25-8" name="__codelineno-25-8" href="#__codelineno-25-8"></a>
<a id="__codelineno-25-9" name="__codelineno-25-9" href="#__codelineno-25-9"></a># Placeholders used for capture
<a id="__codelineno-25-10" name="__codelineno-25-10" href="#__codelineno-25-10"></a>static_input = torch.randn(N, D_in, device=&#39;cuda&#39;)
<a id="__codelineno-25-11" name="__codelineno-25-11" href="#__codelineno-25-11"></a>static_target = torch.randn(N, D_out, device=&#39;cuda&#39;)
<a id="__codelineno-25-12" name="__codelineno-25-12" href="#__codelineno-25-12"></a>
<a id="__codelineno-25-13" name="__codelineno-25-13" href="#__codelineno-25-13"></a># warmup
<a id="__codelineno-25-14" name="__codelineno-25-14" href="#__codelineno-25-14"></a># Uses static_input and static_target here for convenience,
<a id="__codelineno-25-15" name="__codelineno-25-15" href="#__codelineno-25-15"></a># but in a real setting, because the warmup includes optimizer.step()
<a id="__codelineno-25-16" name="__codelineno-25-16" href="#__codelineno-25-16"></a># you must use a few batches of real data.
<a id="__codelineno-25-17" name="__codelineno-25-17" href="#__codelineno-25-17"></a>s = torch.cuda.Stream()
<a id="__codelineno-25-18" name="__codelineno-25-18" href="#__codelineno-25-18"></a>s.wait_stream(torch.cuda.current_stream())
<a id="__codelineno-25-19" name="__codelineno-25-19" href="#__codelineno-25-19"></a>with torch.cuda.stream(s):
<a id="__codelineno-25-20" name="__codelineno-25-20" href="#__codelineno-25-20"></a>    for i in range(3):
<a id="__codelineno-25-21" name="__codelineno-25-21" href="#__codelineno-25-21"></a>        optimizer.zero_grad(set_to_none=True)
<a id="__codelineno-25-22" name="__codelineno-25-22" href="#__codelineno-25-22"></a>        y_pred = model(static_input)
<a id="__codelineno-25-23" name="__codelineno-25-23" href="#__codelineno-25-23"></a>        loss = loss_fn(y_pred, static_target)
<a id="__codelineno-25-24" name="__codelineno-25-24" href="#__codelineno-25-24"></a>        loss.backward()
<a id="__codelineno-25-25" name="__codelineno-25-25" href="#__codelineno-25-25"></a>        optimizer.step()
<a id="__codelineno-25-26" name="__codelineno-25-26" href="#__codelineno-25-26"></a>torch.cuda.current_stream().wait_stream(s)
<a id="__codelineno-25-27" name="__codelineno-25-27" href="#__codelineno-25-27"></a>
<a id="__codelineno-25-28" name="__codelineno-25-28" href="#__codelineno-25-28"></a># capture
<a id="__codelineno-25-29" name="__codelineno-25-29" href="#__codelineno-25-29"></a>g = torch.cuda.CUDAGraph()
<a id="__codelineno-25-30" name="__codelineno-25-30" href="#__codelineno-25-30"></a># Sets grads to None before capture, so backward() will create
<a id="__codelineno-25-31" name="__codelineno-25-31" href="#__codelineno-25-31"></a># .grad attributes with allocations from the graph&#39;s private pool
<a id="__codelineno-25-32" name="__codelineno-25-32" href="#__codelineno-25-32"></a>optimizer.zero_grad(set_to_none=True)
<a id="__codelineno-25-33" name="__codelineno-25-33" href="#__codelineno-25-33"></a>with torch.cuda.graph(g):
<a id="__codelineno-25-34" name="__codelineno-25-34" href="#__codelineno-25-34"></a>    static_y_pred = model(static_input)
<a id="__codelineno-25-35" name="__codelineno-25-35" href="#__codelineno-25-35"></a>    static_loss = loss_fn(static_y_pred, static_target)
<a id="__codelineno-25-36" name="__codelineno-25-36" href="#__codelineno-25-36"></a>    static_loss.backward()
<a id="__codelineno-25-37" name="__codelineno-25-37" href="#__codelineno-25-37"></a>    optimizer.step()
<a id="__codelineno-25-38" name="__codelineno-25-38" href="#__codelineno-25-38"></a>
<a id="__codelineno-25-39" name="__codelineno-25-39" href="#__codelineno-25-39"></a>real_inputs = [torch.rand_like(static_input) for _ in range(10)]
<a id="__codelineno-25-40" name="__codelineno-25-40" href="#__codelineno-25-40"></a>real_targets = [torch.rand_like(static_target) for _ in range(10)]
<a id="__codelineno-25-41" name="__codelineno-25-41" href="#__codelineno-25-41"></a>
<a id="__codelineno-25-42" name="__codelineno-25-42" href="#__codelineno-25-42"></a>for data, target in zip(real_inputs, real_targets):
<a id="__codelineno-25-43" name="__codelineno-25-43" href="#__codelineno-25-43"></a>    # Fills the graph&#39;s input memory with new data to compute on
<a id="__codelineno-25-44" name="__codelineno-25-44" href="#__codelineno-25-44"></a>    static_input.copy_(data)
<a id="__codelineno-25-45" name="__codelineno-25-45" href="#__codelineno-25-45"></a>    static_target.copy_(target)
<a id="__codelineno-25-46" name="__codelineno-25-46" href="#__codelineno-25-46"></a>    # replay() includes forward, backward, and step.
<a id="__codelineno-25-47" name="__codelineno-25-47" href="#__codelineno-25-47"></a>    # You don&#39;t even need to call optimizer.zero_grad() between iterations
<a id="__codelineno-25-48" name="__codelineno-25-48" href="#__codelineno-25-48"></a>    # because the captured backward refills static .grad tensors in place.
<a id="__codelineno-25-49" name="__codelineno-25-49" href="#__codelineno-25-49"></a>    g.replay()
<a id="__codelineno-25-50" name="__codelineno-25-50" href="#__codelineno-25-50"></a>    # Params have been updated. static_y_pred, static_loss, and .grad
<a id="__codelineno-25-51" name="__codelineno-25-51" href="#__codelineno-25-51"></a>    # attributes hold values from computing on this iteration&#39;s data.
</code></pre></div>
<h3 id="_12">部分网络捕获 <a href="#partial-network-capture" title="此标题的固定链接">¶</a></h3>
<p>如果您的某些网络无法安全捕获(例如，由于动态控制流、动态形状、CPU 同步或基本的 CPU 端逻辑)，您可以急切地运行不安全部分并使用 <a href="../generated/torch.cuda.make_graphed_callables.html#torch.cuda.make_graphed_callables" title="torch.cuda.make_graphed_callables"><code>torch.cuda.make _graphed_callables()</code></a> 仅绘制捕获安全部分的图形。</p>
<p>默认情况下， <a href="../generated/torch.cuda.make_graphed_callables.html#torch.cuda.make_graphed_callables" title="torch.cuda.make_graphed_callables"><code>make_graphed_callables()</code></a> 返回的可调用对象是自动分级感知的，并且可以在训练循环中用作您传递的函数或 <a href="../generated/torch.nn.Module.html#torch.nn.Module" title="torch.nn.Module"><code>nn.Module</code></a> 的直接替换。</p>
<p><a href="../generated/torch.cuda.make_graphed_callables.html#torch.cuda.make_graphed_callables" title="torch.cuda.make_graphed_callables"><code>make_graphed_callables()</code></a> 内部创建 <a href="../generated /torch.cuda.CUDAGraph.html#torch.cuda.CUDAGraph" title="torch.cuda.CUDAGraph"><code>CUDAGraph</code></a> 对象，运行预热迭代，并根据需要维护静态输入和输出。因此(与 <a href="../generated/torch.cuda.graph.html#torch.cuda.graph" title="torch.cuda.graph"><code>torch.cuda.graph</code></a> 不同)您不需要手动处理这些。</p>
<p>在下面的示例中，依赖于数据的动态控制流意味着网络无法端到端捕获，但是 <a href="../generated/torch.cuda.make_graphed_callables.html#torch.cuda.make_graphed_callables" title="torch.cuda.make_graphed_callables"><code>make_graphed_callables()</code></a> 让我们能够以图形形式捕获和运行图形安全部分，无论：</p>
<div class="highlight"><pre><span></span><code><a id="__codelineno-26-1" name="__codelineno-26-1" href="#__codelineno-26-1"></a>N, D_in, H, D_out = 640, 4096, 2048, 1024
<a id="__codelineno-26-2" name="__codelineno-26-2" href="#__codelineno-26-2"></a>
<a id="__codelineno-26-3" name="__codelineno-26-3" href="#__codelineno-26-3"></a>module1 = torch.nn.Linear(D_in, H).cuda()
<a id="__codelineno-26-4" name="__codelineno-26-4" href="#__codelineno-26-4"></a>module2 = torch.nn.Linear(H, D_out).cuda()
<a id="__codelineno-26-5" name="__codelineno-26-5" href="#__codelineno-26-5"></a>module3 = torch.nn.Linear(H, D_out).cuda()
<a id="__codelineno-26-6" name="__codelineno-26-6" href="#__codelineno-26-6"></a>
<a id="__codelineno-26-7" name="__codelineno-26-7" href="#__codelineno-26-7"></a>loss_fn = torch.nn.MSELoss()
<a id="__codelineno-26-8" name="__codelineno-26-8" href="#__codelineno-26-8"></a>optimizer = torch.optim.SGD(chain(module1.parameters(),
<a id="__codelineno-26-9" name="__codelineno-26-9" href="#__codelineno-26-9"></a>                                  module2.parameters(),
<a id="__codelineno-26-10" name="__codelineno-26-10" href="#__codelineno-26-10"></a>                                  module3.parameters()),
<a id="__codelineno-26-11" name="__codelineno-26-11" href="#__codelineno-26-11"></a>                            lr=0.1)
<a id="__codelineno-26-12" name="__codelineno-26-12" href="#__codelineno-26-12"></a>
<a id="__codelineno-26-13" name="__codelineno-26-13" href="#__codelineno-26-13"></a># Sample inputs used for capture
<a id="__codelineno-26-14" name="__codelineno-26-14" href="#__codelineno-26-14"></a># requires_grad state of sample inputs must match
<a id="__codelineno-26-15" name="__codelineno-26-15" href="#__codelineno-26-15"></a># requires_grad state of real inputs each callable will see.
<a id="__codelineno-26-16" name="__codelineno-26-16" href="#__codelineno-26-16"></a>x = torch.randn(N, D_in, device=&#39;cuda&#39;)
<a id="__codelineno-26-17" name="__codelineno-26-17" href="#__codelineno-26-17"></a>h = torch.randn(N, H, device=&#39;cuda&#39;, requires_grad=True)
<a id="__codelineno-26-18" name="__codelineno-26-18" href="#__codelineno-26-18"></a>
<a id="__codelineno-26-19" name="__codelineno-26-19" href="#__codelineno-26-19"></a>module1 = torch.cuda.make_graphed_callables(module1, (x,))
<a id="__codelineno-26-20" name="__codelineno-26-20" href="#__codelineno-26-20"></a>module2 = torch.cuda.make_graphed_callables(module2, (h,))
<a id="__codelineno-26-21" name="__codelineno-26-21" href="#__codelineno-26-21"></a>module3 = torch.cuda.make_graphed_callables(module3, (h,))
<a id="__codelineno-26-22" name="__codelineno-26-22" href="#__codelineno-26-22"></a>
<a id="__codelineno-26-23" name="__codelineno-26-23" href="#__codelineno-26-23"></a>real_inputs = [torch.rand_like(x) for _ in range(10)]
<a id="__codelineno-26-24" name="__codelineno-26-24" href="#__codelineno-26-24"></a>real_targets = [torch.randn(N, D_out, device=&quot;cuda&quot;) for _ in range(10)]
<a id="__codelineno-26-25" name="__codelineno-26-25" href="#__codelineno-26-25"></a>
<a id="__codelineno-26-26" name="__codelineno-26-26" href="#__codelineno-26-26"></a>for data, target in zip(real_inputs, real_targets):
<a id="__codelineno-26-27" name="__codelineno-26-27" href="#__codelineno-26-27"></a>    optimizer.zero_grad(set_to_none=True)
<a id="__codelineno-26-28" name="__codelineno-26-28" href="#__codelineno-26-28"></a>
<a id="__codelineno-26-29" name="__codelineno-26-29" href="#__codelineno-26-29"></a>    tmp = module1(data)  # forward ops run as a graph
<a id="__codelineno-26-30" name="__codelineno-26-30" href="#__codelineno-26-30"></a>
<a id="__codelineno-26-31" name="__codelineno-26-31" href="#__codelineno-26-31"></a>    if tmp.sum().item() &gt; 0:
<a id="__codelineno-26-32" name="__codelineno-26-32" href="#__codelineno-26-32"></a>        tmp = module2(tmp)  # forward ops run as a graph
<a id="__codelineno-26-33" name="__codelineno-26-33" href="#__codelineno-26-33"></a>    else:
<a id="__codelineno-26-34" name="__codelineno-26-34" href="#__codelineno-26-34"></a>        tmp = module3(tmp)  # forward ops run as a graph
<a id="__codelineno-26-35" name="__codelineno-26-35" href="#__codelineno-26-35"></a>
<a id="__codelineno-26-36" name="__codelineno-26-36" href="#__codelineno-26-36"></a>    loss = loss_fn(tmp, target)
<a id="__codelineno-26-37" name="__codelineno-26-37" href="#__codelineno-26-37"></a>    # module2&#39;s or module3&#39;s (whichever was chosen) backward ops,
<a id="__codelineno-26-38" name="__codelineno-26-38" href="#__codelineno-26-38"></a>    # as well as module1&#39;s backward ops, run as graphs
<a id="__codelineno-26-39" name="__codelineno-26-39" href="#__codelineno-26-39"></a>    loss.backward()
<a id="__codelineno-26-40" name="__codelineno-26-40" href="#__codelineno-26-40"></a>    optimizer.step()
</code></pre></div>
<h3 id="torchcudaamp">使用 torch.cuda.amp <a href="#usage-with-torch-cuda-amp" title="永久链接到此标题">¶</a></h3>
<p>对于典型的优化器，<a href="../amp.html#torch.cuda.amp.GradScaler.step" title="torch.cuda.amp.GradScaler.step"><code>GradScaler.step</code></a> 将 CPU 与 GPU 同步，这是在捕获。为了避免错误，请使用 <a href="#partial-network-capture">partial-network capture</a> ，或者(如果前向、损失和后向是捕获安全的)捕获前向、损失和后向，但不捕获优化器步骤：</p>
<div class="highlight"><pre><span></span><code><a id="__codelineno-27-1" name="__codelineno-27-1" href="#__codelineno-27-1"></a># warmup
<a id="__codelineno-27-2" name="__codelineno-27-2" href="#__codelineno-27-2"></a># In a real setting, use a few batches of real data.
<a id="__codelineno-27-3" name="__codelineno-27-3" href="#__codelineno-27-3"></a>s = torch.cuda.Stream()
<a id="__codelineno-27-4" name="__codelineno-27-4" href="#__codelineno-27-4"></a>s.wait_stream(torch.cuda.current_stream())
<a id="__codelineno-27-5" name="__codelineno-27-5" href="#__codelineno-27-5"></a>with torch.cuda.stream(s):
<a id="__codelineno-27-6" name="__codelineno-27-6" href="#__codelineno-27-6"></a>    for i in range(3):
<a id="__codelineno-27-7" name="__codelineno-27-7" href="#__codelineno-27-7"></a>        optimizer.zero_grad(set_to_none=True)
<a id="__codelineno-27-8" name="__codelineno-27-8" href="#__codelineno-27-8"></a>        with torch.cuda.amp.autocast():
<a id="__codelineno-27-9" name="__codelineno-27-9" href="#__codelineno-27-9"></a>            y_pred = model(static_input)
<a id="__codelineno-27-10" name="__codelineno-27-10" href="#__codelineno-27-10"></a>            loss = loss_fn(y_pred, static_target)
<a id="__codelineno-27-11" name="__codelineno-27-11" href="#__codelineno-27-11"></a>        scaler.scale(loss).backward()
<a id="__codelineno-27-12" name="__codelineno-27-12" href="#__codelineno-27-12"></a>        scaler.step(optimizer)
<a id="__codelineno-27-13" name="__codelineno-27-13" href="#__codelineno-27-13"></a>        scaler.update()
<a id="__codelineno-27-14" name="__codelineno-27-14" href="#__codelineno-27-14"></a>torch.cuda.current_stream().wait_stream(s)
<a id="__codelineno-27-15" name="__codelineno-27-15" href="#__codelineno-27-15"></a>
<a id="__codelineno-27-16" name="__codelineno-27-16" href="#__codelineno-27-16"></a># capture
<a id="__codelineno-27-17" name="__codelineno-27-17" href="#__codelineno-27-17"></a>g = torch.cuda.CUDAGraph()
<a id="__codelineno-27-18" name="__codelineno-27-18" href="#__codelineno-27-18"></a>optimizer.zero_grad(set_to_none=True)
<a id="__codelineno-27-19" name="__codelineno-27-19" href="#__codelineno-27-19"></a>with torch.cuda.graph(g):
<a id="__codelineno-27-20" name="__codelineno-27-20" href="#__codelineno-27-20"></a>    with torch.cuda.amp.autocast():
<a id="__codelineno-27-21" name="__codelineno-27-21" href="#__codelineno-27-21"></a>        static_y_pred = model(static_input)
<a id="__codelineno-27-22" name="__codelineno-27-22" href="#__codelineno-27-22"></a>        static_loss = loss_fn(static_y_pred, static_target)
<a id="__codelineno-27-23" name="__codelineno-27-23" href="#__codelineno-27-23"></a>    scaler.scale(static_loss).backward()
<a id="__codelineno-27-24" name="__codelineno-27-24" href="#__codelineno-27-24"></a>    # don&#39;t capture scaler.step(optimizer) or scaler.update()
<a id="__codelineno-27-25" name="__codelineno-27-25" href="#__codelineno-27-25"></a>
<a id="__codelineno-27-26" name="__codelineno-27-26" href="#__codelineno-27-26"></a>real_inputs = [torch.rand_like(static_input) for _ in range(10)]
<a id="__codelineno-27-27" name="__codelineno-27-27" href="#__codelineno-27-27"></a>real_targets = [torch.rand_like(static_target) for _ in range(10)]
<a id="__codelineno-27-28" name="__codelineno-27-28" href="#__codelineno-27-28"></a>
<a id="__codelineno-27-29" name="__codelineno-27-29" href="#__codelineno-27-29"></a>for data, target in zip(real_inputs, real_targets):
<a id="__codelineno-27-30" name="__codelineno-27-30" href="#__codelineno-27-30"></a>    static_input.copy_(data)
<a id="__codelineno-27-31" name="__codelineno-27-31" href="#__codelineno-27-31"></a>    static_target.copy_(target)
<a id="__codelineno-27-32" name="__codelineno-27-32" href="#__codelineno-27-32"></a>    g.replay()
<a id="__codelineno-27-33" name="__codelineno-27-33" href="#__codelineno-27-33"></a>    # Runs scaler.step and scaler.update eagerly
<a id="__codelineno-27-34" name="__codelineno-27-34" href="#__codelineno-27-34"></a>    scaler.step(optimizer)
<a id="__codelineno-27-35" name="__codelineno-27-35" href="#__codelineno-27-35"></a>    scaler.update()
</code></pre></div>
<h3 id="_13">与多个流一起使用 <a href="#usage-with-multiple-streams" title="此标题的永久链接">¶</a></h3>
<p>捕获模式自动传播到与捕获流同步的任何流。在捕获中，您可以通过向不同流发出调用来公开并行性，但总体流依赖性 DAG 必须在捕获开始后从初始捕获流中分支出来，并在捕获之前重新加入初始流结束：</p>
<div class="highlight"><pre><span></span><code><a id="__codelineno-28-1" name="__codelineno-28-1" href="#__codelineno-28-1"></a>with torch.cuda.graph(g):
<a id="__codelineno-28-2" name="__codelineno-28-2" href="#__codelineno-28-2"></a>    # at context manager entrance, torch.cuda.current_stream()
<a id="__codelineno-28-3" name="__codelineno-28-3" href="#__codelineno-28-3"></a>    # is the initial capturing stream
<a id="__codelineno-28-4" name="__codelineno-28-4" href="#__codelineno-28-4"></a>
<a id="__codelineno-28-5" name="__codelineno-28-5" href="#__codelineno-28-5"></a>    # INCORRECT (does not branch out from or rejoin initial stream)
<a id="__codelineno-28-6" name="__codelineno-28-6" href="#__codelineno-28-6"></a>    with torch.cuda.stream(s):
<a id="__codelineno-28-7" name="__codelineno-28-7" href="#__codelineno-28-7"></a>        cuda_work()
<a id="__codelineno-28-8" name="__codelineno-28-8" href="#__codelineno-28-8"></a>
<a id="__codelineno-28-9" name="__codelineno-28-9" href="#__codelineno-28-9"></a>    # CORRECT:
<a id="__codelineno-28-10" name="__codelineno-28-10" href="#__codelineno-28-10"></a>    # branches out from initial stream
<a id="__codelineno-28-11" name="__codelineno-28-11" href="#__codelineno-28-11"></a>    s.wait_stream(torch.cuda.current_stream())
<a id="__codelineno-28-12" name="__codelineno-28-12" href="#__codelineno-28-12"></a>    with torch.cuda.stream(s):
<a id="__codelineno-28-13" name="__codelineno-28-13" href="#__codelineno-28-13"></a>        cuda_work()
<a id="__codelineno-28-14" name="__codelineno-28-14" href="#__codelineno-28-14"></a>    # rejoins initial stream before capture ends
<a id="__codelineno-28-15" name="__codelineno-28-15" href="#__codelineno-28-15"></a>    torch.cuda.current_stream().wait_stream(s)
</code></pre></div>
<div class="admonition note">
<p class="admonition-title">笔记</p>
<p>为了避免高级用户在 nsight 系统或 nvprof 中查看重播时感到困惑：与急切执行不同，该图将 capture 中的重要流 DAG 解释为提示，而不是命令。在重放期间，图表可能会将独立的 opson 重新组织到不同的流或以不同的顺序将它们排入队列(同时尊重原始 DAG 的整体依赖性)。</p>
</div>
<h3 id="distributeddataparallel">与 DistributedDataParallel 的用法 <a href="#usage-with-distributeddataparallel" title="此标题的永久链接">¶</a></h3>
<h4 id="nccl-296">NCCL &lt; 2.9.6 <a href="#nccl-2-9-6“此标题的永久链接”">¶</a></h4>
<p>早于 2.9.6 的 NCCL 版本不允许捕获集合。您必须使用 <a href="#partial-network-capture">partial-network capture</a> ，这会推迟所有归约发生在向后的图形部分之外。</p>
<p><em>在</em>使用以下命令包装网络之前，在可图形网络部分上调用 <a href="../generated/torch.cuda.make_graphed_callables.html#torch.cuda.make_graphed_callables" title="torch.cuda.make_graphed_callables"><code>make_graphed_callables()</code></a>顺铂。</p>
<h4 id="nccl-296_1">NCCL &gt;= 2.9.6 <a href="#id5“此标题的永久链接”">¶</a></h4>
<p>NCCL 版本 2.9.6 或更高版本允许在图中进行集合。捕获<a href="#whole-network-capture">整个向后传递</a> 的方法是一个可行的选项，但需要三个设置步骤。</p>
<ol>
<li>禁用DDP的内部异步错误处理：</li>
</ol>
<p><div class="highlight"><pre><span></span><code><a id="__codelineno-29-1" name="__codelineno-29-1" href="#__codelineno-29-1"></a>os.environ[&quot;NCCL_ASYNC_ERROR_HANDLING&quot;] = &quot;0&quot;
<a id="__codelineno-29-2" name="__codelineno-29-2" href="#__codelineno-29-2"></a>torch.distributed.init_process_group(...)
</code></pre></div>
2. Before full-backward capture, DDP must be constructed in a side-stream context:</p>
<p><div class="highlight"><pre><span></span><code><a id="__codelineno-30-1" name="__codelineno-30-1" href="#__codelineno-30-1"></a>with torch.cuda.stream(s):
<a id="__codelineno-30-2" name="__codelineno-30-2" href="#__codelineno-30-2"></a>    model = DistributedDataParallel(model)
</code></pre></div>
3. Your warmup must run at least 11 DDP-enabled eager iterations before capture.</p>
<h3 id="_14">图形内存管理 <a href="#graph-memory-management" title="永久链接到此标题">¶</a></h3>
<p>捕获的图每次重放时都会作用于相同的虚拟地址。如果 PyTorch 释放内存，则稍后的重放可能会遇到非法内存访问。如果 PyTorch 将内存重新分配给新的tensor，则重放可能会破坏这些tensor看到的值。因此，必须为跨重放的图保留图使用的虚拟地址。 PyTorch 缓存分配器通过检测何时进行捕获并满足来自图形专用内存池的捕获分配来实现此目的。私有池将保持活动状态，直到其 <a href="../generated/torch.cuda.CUDAGraph.html#torch.cuda.CUDAGraph" title="torch.cuda.CUDAGraph"><code>CUDAGraph</code></a> 对象和在 capturego 期间创建的所有tensor超出范围。</p>
<p>私人池是自动维护的。默认情况下，分配器为每个捕获创建单独的专用池。如果您捕获多个图形，这种保守的方法可确保图形重播不会破坏彼此的值，但有时会不必要地浪费内存。</p>
<h4 id="_15">跨捕获共享内存 <a href="#sharing-memory-across-captures" title="永久链接到此标题">¶</a></h4>
<p>为了节省私有池中存储的内存， <a href="../generated/torch.cuda.graph.html#torch.cuda.graph" title="torch.cuda.graph"><code>torch.cuda.graph</code></a> 和 <a href="../generated/torch.cuda.make_graphed_callables.html#torch.cuda.make_graphed_callables" title="torch.cuda.make_graphed_callables"><code>torch. cuda.make_graphed_callables()</code></a> 可选择允许不同的捕获共享相同的私有池。这是安全的对于共享私有池的一组图表，如果您知道它们将始终按照捕获的顺序重播，并且永远不会同时重播。</p>
<p><a href="../generated/torch.cuda.graph.html#torch.cuda.graph" title="torch.cuda.graph"><code>torch.cuda.graph</code></a> 的 <code>pool</code> 参数是使用特定私有的提示池，可用于跨图共享内存，如下所示：</p>
<div class="highlight"><pre><span></span><code><a id="__codelineno-31-1" name="__codelineno-31-1" href="#__codelineno-31-1"></a>g1 = torch.cuda.CUDAGraph()
<a id="__codelineno-31-2" name="__codelineno-31-2" href="#__codelineno-31-2"></a>g2 = torch.cuda.CUDAGraph()
<a id="__codelineno-31-3" name="__codelineno-31-3" href="#__codelineno-31-3"></a>
<a id="__codelineno-31-4" name="__codelineno-31-4" href="#__codelineno-31-4"></a># (create static inputs for g1 and g2, run warmups of their workloads...)
<a id="__codelineno-31-5" name="__codelineno-31-5" href="#__codelineno-31-5"></a>
<a id="__codelineno-31-6" name="__codelineno-31-6" href="#__codelineno-31-6"></a># Captures g1
<a id="__codelineno-31-7" name="__codelineno-31-7" href="#__codelineno-31-7"></a>with torch.cuda.graph(g1):
<a id="__codelineno-31-8" name="__codelineno-31-8" href="#__codelineno-31-8"></a>    static_out_1 = g1_workload(static_in_1)
<a id="__codelineno-31-9" name="__codelineno-31-9" href="#__codelineno-31-9"></a>
<a id="__codelineno-31-10" name="__codelineno-31-10" href="#__codelineno-31-10"></a># Captures g2, hinting that g2 may share a memory pool with g1
<a id="__codelineno-31-11" name="__codelineno-31-11" href="#__codelineno-31-11"></a>with torch.cuda.graph(g2, pool=g1.pool()):
<a id="__codelineno-31-12" name="__codelineno-31-12" href="#__codelineno-31-12"></a>    static_out_2 = g2_workload(static_in_2)
<a id="__codelineno-31-13" name="__codelineno-31-13" href="#__codelineno-31-13"></a>
<a id="__codelineno-31-14" name="__codelineno-31-14" href="#__codelineno-31-14"></a>static_in_1.copy_(real_data_1)
<a id="__codelineno-31-15" name="__codelineno-31-15" href="#__codelineno-31-15"></a>static_in_2.copy_(real_data_2)
<a id="__codelineno-31-16" name="__codelineno-31-16" href="#__codelineno-31-16"></a>g1.replay()
<a id="__codelineno-31-17" name="__codelineno-31-17" href="#__codelineno-31-17"></a>g2.replay()
</code></pre></div>
<p>使用 <a href="../generated/torch.cuda.make_graphed_callables.html#torch.cuda.make_graphed_callables" title="torch.cuda.make_graphed_callables"><code>torch.cuda.make_graphed_callables()</code></a> ，如果你想绘制多个可调用的图形并且您知道它们将始终以相同的顺序运行(并且从不同时)将它们作为元组按照它们在实时工作负载中运行的顺序传递，并且 <a href="../generated/torch.cuda.make_graphed_callables.html#torch.cuda.make_graphed_callables" title="torch.cuda.make_graphed_callables"><code>make_graphed_callables()</code></a> 将使用共享私有池捕获它们的图表。</p>
<p>如果在实时工作负载中，您的可调用对象将以偶尔更改的顺序运行，或者如果它们同时运行，则将它们作为元组传递给 <a href="../generated/torch.cuda.make_graphed_callables.html#torch.cuda.make_graphed_callables" title="torch.cuda.make_graphed_callables"><code>make_graphed_callables()</code></a>是不允许的。相反，您必须为每个单独调用 <a href="../generated/torch.cuda.make_graphed_callables.html#torch.cuda.make_graphed_callables" title="torch.cuda.make_graphed_callables"><code>make_graphed_callables()</code></a> 。</p>












                
              </article>
            </div>
          
          
<script>var target=document.getElementById(location.hash.slice(1));target&&target.name&&(target.checked=target.name.startsWith("__tabbed_"))</script>
        </div>
        
      </main>
      
        <footer class="md-footer">
  
    
      
      <nav class="md-footer__inner md-grid" aria-label="Footer" >
        
          
          <a href="../cpu_threading_torchscript_inference/" class="md-footer__link md-footer__link--prev" aria-label="Previous: CPU threading and TorchScript inference">
            <div class="md-footer__button md-icon">
              
              <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M20 11v2H8l5.5 5.5-1.42 1.42L4.16 12l7.92-7.92L13.5 5.5 8 11h12Z"/></svg>
            </div>
            <div class="md-footer__title">
              <span class="md-footer__direction">
                Previous
              </span>
              <div class="md-ellipsis">
                CPU threading and TorchScript inference
              </div>
            </div>
          </a>
        
        
          
          <a href="../ddp/" class="md-footer__link md-footer__link--next" aria-label="Next: Distributed Data Parallel">
            <div class="md-footer__title">
              <span class="md-footer__direction">
                Next
              </span>
              <div class="md-ellipsis">
                Distributed Data Parallel
              </div>
            </div>
            <div class="md-footer__button md-icon">
              
              <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M4 11v2h12l-5.5 5.5 1.42 1.42L19.84 12l-7.92-7.92L10.5 5.5 16 11H4Z"/></svg>
            </div>
          </a>
        
      </nav>
    
  
  <div class="md-footer-meta md-typeset">
    <div class="md-footer-meta__inner md-grid">
      <div class="md-copyright">
  
  
    Made with
    <a href="https://squidfunk.github.io/mkdocs-material/" target="_blank" rel="noopener">
      Material for MkDocs
    </a>
  
</div>
      
    </div>
  </div>
</footer>
      
    </div>
    <div class="md-dialog" data-md-component="dialog">
      <div class="md-dialog__inner md-typeset"></div>
    </div>
    
    
    <script id="__config" type="application/json">{"base": "../../../..", "features": ["content.code.copy", "content.action.edit", "content.action.view", "navigation.footer"], "search": "../../../../assets/javascripts/workers/search.b8dbb3d2.min.js", "translations": {"clipboard.copied": "Copied to clipboard", "clipboard.copy": "Copy to clipboard", "search.result.more.one": "1 more on this page", "search.result.more.other": "# more on this page", "search.result.none": "No matching documents", "search.result.one": "1 matching document", "search.result.other": "# matching documents", "search.result.placeholder": "Type to start searching", "search.result.term.missing": "Missing", "select.version": "Select version"}}</script>
    
    
      <script src="../../../../assets/javascripts/bundle.8fd75fb4.min.js"></script>
      
        <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
      
        <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
      
    
  </body>
</html>